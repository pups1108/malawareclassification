{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name np_utils",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-51db091c23cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/venv/malawareclassification/lib/python2.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/venv/malawareclassification/lib/python2.7/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name np_utils"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from keras.models import load_model\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from os import listdir\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import RepeatVector\n",
    "from keras.layers import Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import model_from_yaml\n",
    "import os\n",
    "from keras.utils import plot_model\n",
    "#/Users/user/PycharmProjects/malawareclassification/\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "apa = 200\n",
    "batch = 10\n",
    "timestep = 16\n",
    "numfeature = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gothrougheveryfile(dir):  # go through every csv and concat to one csv this part has move to concatallcsv\n",
    "    i = 0\n",
    "    filenamelist = list()\n",
    "    for filename in os.listdir(dir):  # outside each folder\n",
    "        wholefilepath = dir + \"//\" + filename\n",
    "        filenamelist.append(wholefilepath)\n",
    "    return filenamelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_reconstruction_problem(data, timestep):\n",
    "    df = DataFrame(data)\n",
    "    list_concat = list()\n",
    "    for i in range(timestep - 1, -1, -1):\n",
    "        tempdf = df.shift(i)\n",
    "        list_concat.append(tempdf)\n",
    "    data_for_autoencoder = concat(list_concat, axis=1)\n",
    "    data_for_autoencoder.dropna(inplace=True)\n",
    "    return data_for_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_put_core(writting_list, outaddress, filename):\n",
    "    thefile = open(outaddress + \"\\\\\" + filename + \".txt\", 'w')\n",
    "    for item in writting_list:\n",
    "        thefile.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(file_name, timestep):\n",
    "    # read\n",
    "    dataset = read_csv(file_name, header=None, index_col=None)\n",
    "    if dataset.shape[0]<16:\n",
    "        return 0,0\n",
    "    values = dataset.values\n",
    "    reframed = data_to_reconstruction_problem(values, timestep)\n",
    "    reframedvalues = reframed\n",
    "    reframed = reframed.astype('float32')\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled = scaler.fit_transform(reframed)\n",
    "    dfscaled = DataFrame(scaled)\n",
    "    valuescaled =dfscaled.values\n",
    "    return  valuescaled,scaler,reframedvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SingleFileLstmAutoencoder(apa, batch, n_apis, n_features, data_for_model_training):\n",
    "    #error_list = list()\n",
    "    W_Hidden1_list = list()\n",
    "    W_Hidden2_list = list()\n",
    "    W_Hidden3_list = list()\n",
    "    W_Hidden4_list = list()\n",
    "    W_Hidden5_list = list()\n",
    "    train_X, scaler, y = data_preprocess(data_for_model_training, n_apis)#146,400\n",
    "    train_X = train_X.reshape((train_X.shape[0], n_apis, n_features))\n",
    "    sample_number = train_X.shape[0]\n",
    "    outputlayer2 = n_apis #16\n",
    "    outputlayer3 = int(n_apis / 2) #8\n",
    "    timesstep4 = int(n_apis / 4) # 4\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_features, input_shape=(n_apis, train_X.shape[2]), return_sequences=True))  # train_X.shape[2] = 34\n",
    "    model.add(LSTM(outputlayer2, return_sequences=True))\n",
    "    model.add(LSTM(outputlayer3, return_sequences=True))\n",
    "    model.add(LSTM(outputlayer2, return_sequences=True))\n",
    "    model.add(LSTM(n_features, return_sequences=True))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "    history = model.fit(train_X, train_X, epochs=apa, batch_size=batch, shuffle=False)  # train\n",
    "    return model\n",
    "    #model.save(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train5modelAE(apa, batch, timestep, numfeature, filea, fileb, filec, filed, filee):\n",
    "    #SingleFileLstmAutoencoder(apa, batch, n_apis, n_features, data_for_model_training)\n",
    "    k1 = SingleFileLstmAutoencoder(apa, batch, timestep, numfeature, filea)\n",
    "    k2 = SingleFileLstmAutoencoder(apa, batch, timestep, numfeature, fileb)\n",
    "    k3 = SingleFileLstmAutoencoder(apa, batch, timestep, numfeature, filec)\n",
    "    k4 = SingleFileLstmAutoencoder(apa, batch, timestep, numfeature, filed)\n",
    "    k5 = SingleFileLstmAutoencoder(apa, batch, timestep, numfeature, filee)\n",
    "    return k1, k2, k3, k4, k5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postboosting(test_x, timestep, numfeature, apa, batch):\n",
    "    yhatk1 = k1.predict(test_x)\n",
    "    yhatk2 = k2.predict(test_x)\n",
    "    yhatk3 = k3.predict(test_x)\n",
    "    yhatk4 = k4.predict(test_x)\n",
    "    yhatk5 = k5.predict(test_x)\n",
    "    inputofdense = np.concatenate((yhatk1, yhatk2, yhatk3, yhatk4, yhatk5), 1).reshape(test_x.shape[0], 5, numfeature*timestep,1)\n",
    "    test_x = test_x.reshape(test_x.shape[0], 1, numfeature*timestep, 1)\n",
    "    modelmerge = Sequential()\n",
    "    modelmerge.add(Conv2D(input_shape=(5, timestep*numfeature, 1), padding=\"valid\", filters=1, kernel_size=(5, 1)))\n",
    "    modelmerge.compile(loss='mean_squared_error', metrics=['accuracy'], optimizer='adam')\n",
    "    history = modelmerge.fit(inputofdense, test_x,validation_split=0.25,   nb_epoch=apa,  batch_size=batch)\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validate'], loc='upper left')\n",
    "    plt.show()\n",
    "    return modelmerge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "146/146 [==============================] - 7s 45ms/step - loss: 0.2224 - acc: 0.0000e+00\n",
      "Epoch 2/200\n",
      "146/146 [==============================] - 1s 8ms/step - loss: 0.1841 - acc: 0.0000e+00\n",
      "Epoch 3/200\n",
      "146/146 [==============================] - 1s 8ms/step - loss: 0.1282 - acc: 0.0000e+00\n",
      "Epoch 4/200\n",
      "146/146 [==============================] - 1s 8ms/step - loss: 0.0828 - acc: 0.0000e+00\n",
      "Epoch 5/200\n",
      "146/146 [==============================] - 1s 8ms/step - loss: 0.0598 - acc: 0.0000e+00\n",
      "Epoch 6/200\n",
      "146/146 [==============================] - 1s 8ms/step - loss: 0.0483 - acc: 0.0000e+00\n",
      "Epoch 7/200\n",
      "146/146 [==============================] - 1s 8ms/step - loss: 0.0417 - acc: 0.0287\n",
      "Epoch 8/200\n",
      "146/146 [==============================] - 1s 8ms/step - loss: 0.0372 - acc: 0.7247\n",
      "Epoch 9/200\n",
      "146/146 [==============================] - 1s 8ms/step - loss: 0.0342 - acc: 0.2384\n",
      "Epoch 10/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0318 - acc: 0.1866\n",
      "Epoch 11/200\n",
      "146/146 [==============================] - 1s 8ms/step - loss: 0.0299 - acc: 0.1789\n",
      "Epoch 12/200\n",
      "146/146 [==============================] - 1s 8ms/step - loss: 0.0282 - acc: 0.1417\n",
      "Epoch 13/200\n",
      "146/146 [==============================] - 1s 8ms/step - loss: 0.0265 - acc: 0.1259\n",
      "Epoch 14/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0250 - acc: 0.1259\n",
      "Epoch 15/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0235 - acc: 0.1259\n",
      "Epoch 16/200\n",
      "146/146 [==============================] - 1s 8ms/step - loss: 0.0224 - acc: 0.0878\n",
      "Epoch 17/200\n",
      "146/146 [==============================] - 1s 8ms/step - loss: 0.0216 - acc: 0.0732\n",
      "Epoch 18/200\n",
      "146/146 [==============================] - 1s 8ms/step - loss: 0.0210 - acc: 0.0732\n",
      "Epoch 19/200\n",
      "146/146 [==============================] - 1s 8ms/step - loss: 0.0204 - acc: 0.0732\n",
      "Epoch 20/200\n",
      "146/146 [==============================] - 1s 8ms/step - loss: 0.0199 - acc: 0.0732\n",
      "Epoch 21/200\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 0.0195 - acc: 0.0732\n",
      "Epoch 22/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0191 - acc: 0.0732\n",
      "Epoch 23/200\n",
      "146/146 [==============================] - 1s 9ms/step - loss: 0.0187 - acc: 0.0732\n",
      "Epoch 24/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0184 - acc: 0.0732\n",
      "Epoch 25/200\n",
      "146/146 [==============================] - 1s 8ms/step - loss: 0.0181 - acc: 0.0732\n",
      "Epoch 26/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0178 - acc: 0.0732\n",
      "Epoch 27/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0176 - acc: 0.0732\n",
      "Epoch 28/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0174 - acc: 0.0732\n",
      "Epoch 29/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0172 - acc: 0.0248\n",
      "Epoch 30/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0171 - acc: 0.0210\n",
      "Epoch 31/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0169 - acc: 0.0210\n",
      "Epoch 32/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0168 - acc: 0.0210\n",
      "Epoch 33/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0167 - acc: 0.0210\n",
      "Epoch 34/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0166 - acc: 0.0210\n",
      "Epoch 35/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0165 - acc: 0.0210\n",
      "Epoch 36/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0164 - acc: 0.0210\n",
      "Epoch 37/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0164 - acc: 0.0210\n",
      "Epoch 38/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0210\n",
      "Epoch 39/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0210\n",
      "Epoch 40/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0162 - acc: 0.0210\n",
      "Epoch 41/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0162 - acc: 0.0210\n",
      "Epoch 42/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0161 - acc: 0.0210\n",
      "Epoch 43/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0161 - acc: 0.0210\n",
      "Epoch 44/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0160 - acc: 0.0210\n",
      "Epoch 45/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0160 - acc: 0.0210\n",
      "Epoch 46/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0159 - acc: 0.0210\n",
      "Epoch 47/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0159 - acc: 0.0210\n",
      "Epoch 48/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0158 - acc: 0.0210\n",
      "Epoch 49/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0158 - acc: 0.0210\n",
      "Epoch 50/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0158 - acc: 0.0210\n",
      "Epoch 51/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0158 - acc: 0.0210\n",
      "Epoch 52/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0157 - acc: 0.0210\n",
      "Epoch 53/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0157 - acc: 0.0210\n",
      "Epoch 54/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0157 - acc: 0.0210\n",
      "Epoch 55/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0157 - acc: 0.0210\n",
      "Epoch 56/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.0210\n",
      "Epoch 57/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.0210\n",
      "Epoch 58/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.0210\n",
      "Epoch 59/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.0210\n",
      "Epoch 60/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.0210\n",
      "Epoch 61/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0155 - acc: 0.0210\n",
      "Epoch 62/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0155 - acc: 0.0210\n",
      "Epoch 63/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0155 - acc: 0.0210\n",
      "Epoch 64/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0155 - acc: 0.0210\n",
      "Epoch 65/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0155 - acc: 0.0210\n",
      "Epoch 66/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0155 - acc: 0.0210\n",
      "Epoch 67/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0154 - acc: 0.0210\n",
      "Epoch 68/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0154 - acc: 0.0210\n",
      "Epoch 69/200\n",
      "146/146 [==============================] - 1s 6ms/step - loss: 0.0154 - acc: 0.0210\n",
      "Epoch 70/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0154 - acc: 0.0210\n",
      "Epoch 71/200\n",
      "146/146 [==============================] - 1s 6ms/step - loss: 0.0154 - acc: 0.0210\n",
      "Epoch 72/200\n",
      "146/146 [==============================] - 1s 6ms/step - loss: 0.0154 - acc: 0.0210\n",
      "Epoch 73/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0153 - acc: 0.0210\n",
      "Epoch 74/200\n",
      "146/146 [==============================] - 1s 6ms/step - loss: 0.0153 - acc: 0.0210\n",
      "Epoch 75/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0153 - acc: 0.0210\n",
      "Epoch 76/200\n",
      "146/146 [==============================] - 1s 6ms/step - loss: 0.0153 - acc: 0.0210\n",
      "Epoch 77/200\n",
      "146/146 [==============================] - 1s 6ms/step - loss: 0.0153 - acc: 0.0210\n",
      "Epoch 78/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0153 - acc: 0.0210\n",
      "Epoch 79/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0152 - acc: 0.0210\n",
      "Epoch 80/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0152 - acc: 0.0210\n",
      "Epoch 81/200\n",
      "146/146 [==============================] - 1s 6ms/step - loss: 0.0152 - acc: 0.0488\n",
      "Epoch 82/200\n",
      "146/146 [==============================] - 1s 6ms/step - loss: 0.0152 - acc: 0.0736\n",
      "Epoch 83/200\n",
      "146/146 [==============================] - 1s 6ms/step - loss: 0.0152 - acc: 0.0736\n",
      "Epoch 84/200\n",
      "146/146 [==============================] - 1s 6ms/step - loss: 0.0152 - acc: 0.0736\n",
      "Epoch 85/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0152 - acc: 0.0736\n",
      "Epoch 86/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0152 - acc: 0.0736\n",
      "Epoch 87/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0151 - acc: 0.0736\n",
      "Epoch 88/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0151 - acc: 0.0736\n",
      "Epoch 89/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0151 - acc: 0.0736\n",
      "Epoch 90/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0151 - acc: 0.0736\n",
      "Epoch 91/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0151 - acc: 0.0736\n",
      "Epoch 92/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0151 - acc: 0.0736\n",
      "Epoch 93/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0151 - acc: 0.0736\n",
      "Epoch 94/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0151 - acc: 0.0736\n",
      "Epoch 95/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0151 - acc: 0.0736\n",
      "Epoch 96/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0150 - acc: 0.0736\n",
      "Epoch 97/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0150 - acc: 0.0732\n",
      "Epoch 98/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0150 - acc: 0.0732\n",
      "Epoch 99/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0150 - acc: 0.0732\n",
      "Epoch 100/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0150 - acc: 0.0732\n",
      "Epoch 101/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0150 - acc: 0.0732\n",
      "Epoch 102/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0150 - acc: 0.0732\n",
      "Epoch 103/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0150 - acc: 0.0732\n",
      "Epoch 104/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0150 - acc: 0.0732\n",
      "Epoch 105/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0149 - acc: 0.0732A: 0s - loss: 0.0185 - acc: \n",
      "Epoch 106/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0149 - acc: 0.0732\n",
      "Epoch 107/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0149 - acc: 0.0732\n",
      "Epoch 108/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0149 - acc: 0.0732\n",
      "Epoch 109/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0149 - acc: 0.0732\n",
      "Epoch 110/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0149 - acc: 0.0732\n",
      "Epoch 111/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0149 - acc: 0.0732\n",
      "Epoch 112/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0149 - acc: 0.0732\n",
      "Epoch 113/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0149 - acc: 0.0732\n",
      "Epoch 114/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0149 - acc: 0.0732\n",
      "Epoch 115/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0149 - acc: 0.0732\n",
      "Epoch 116/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0148 - acc: 0.0732\n",
      "Epoch 117/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0148 - acc: 0.0732\n",
      "Epoch 118/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0148 - acc: 0.0732\n",
      "Epoch 119/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0148 - acc: 0.0732\n",
      "Epoch 120/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0148 - acc: 0.0732\n",
      "Epoch 121/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0148 - acc: 0.0732\n",
      "Epoch 122/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0148 - acc: 0.0732\n",
      "Epoch 123/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0148 - acc: 0.0732\n",
      "Epoch 124/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0148 - acc: 0.0732\n",
      "Epoch 125/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0148 - acc: 0.0732\n",
      "Epoch 126/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0148 - acc: 0.0732\n",
      "Epoch 127/200\n",
      "146/146 [==============================] - 1s 6ms/step - loss: 0.0148 - acc: 0.0732\n",
      "Epoch 128/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0148 - acc: 0.0732\n",
      "Epoch 129/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0148 - acc: 0.0732\n",
      "Epoch 130/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0148 - acc: 0.0736\n",
      "Epoch 131/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0148 - acc: 0.0736\n",
      "Epoch 132/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0148 - acc: 0.0736\n",
      "Epoch 133/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0148 - acc: 0.0736\n",
      "Epoch 134/200\n",
      "146/146 [==============================] - 1s 8ms/step - loss: 0.0147 - acc: 0.0736\n",
      "Epoch 135/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0736\n",
      "Epoch 136/200\n",
      "146/146 [==============================] - 1s 8ms/step - loss: 0.0147 - acc: 0.0736\n",
      "Epoch 137/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0736\n",
      "Epoch 138/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0736\n",
      "Epoch 139/200\n",
      "146/146 [==============================] - 1s 8ms/step - loss: 0.0147 - acc: 0.0736\n",
      "Epoch 140/200\n",
      "146/146 [==============================] - 1s 8ms/step - loss: 0.0147 - acc: 0.0736\n",
      "Epoch 141/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0736\n",
      "Epoch 142/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0736\n",
      "Epoch 143/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0736\n",
      "Epoch 144/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0736\n",
      "Epoch 145/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0736\n",
      "Epoch 146/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0736\n",
      "Epoch 147/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0736\n",
      "Epoch 148/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0741\n",
      "Epoch 149/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0741\n",
      "Epoch 150/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0741\n",
      "Epoch 151/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0741\n",
      "Epoch 152/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0741\n",
      "Epoch 153/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0741\n",
      "Epoch 154/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0741\n",
      "Epoch 155/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0741\n",
      "Epoch 156/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0741\n",
      "Epoch 157/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0741\n",
      "Epoch 158/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0741\n",
      "Epoch 159/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0741\n",
      "Epoch 160/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0741\n",
      "Epoch 161/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0741\n",
      "Epoch 162/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0741\n",
      "Epoch 163/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0741\n",
      "Epoch 164/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0741A: 0s - loss: 0.0152 - acc: 0.0\n",
      "Epoch 165/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0741\n",
      "Epoch 166/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0741\n",
      "Epoch 167/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0741\n",
      "Epoch 168/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0741\n",
      "Epoch 169/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0741A: 0s - loss: 0.0161 - acc: 0.\n",
      "Epoch 170/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0741\n",
      "Epoch 171/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0147 - acc: 0.0741\n",
      "Epoch 172/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0741\n",
      "Epoch 173/200\n",
      "146/146 [==============================] - 1s 6ms/step - loss: 0.0146 - acc: 0.0741\n",
      "Epoch 174/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0741\n",
      "Epoch 175/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0741\n",
      "Epoch 176/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0741\n",
      "Epoch 177/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0741\n",
      "Epoch 178/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0736\n",
      "Epoch 179/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0736\n",
      "Epoch 180/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0736\n",
      "Epoch 181/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0736\n",
      "Epoch 182/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0736\n",
      "Epoch 183/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0736\n",
      "Epoch 184/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0736\n",
      "Epoch 185/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0736\n",
      "Epoch 186/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0736\n",
      "Epoch 187/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0736\n",
      "Epoch 188/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0736\n",
      "Epoch 189/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0728\n",
      "Epoch 190/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0732\n",
      "Epoch 191/200\n",
      "146/146 [==============================] - 1s 6ms/step - loss: 0.0146 - acc: 0.0732\n",
      "Epoch 192/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0732\n",
      "Epoch 193/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0732\n",
      "Epoch 194/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0732\n",
      "Epoch 195/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0732\n",
      "Epoch 196/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0732\n",
      "Epoch 197/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0728\n",
      "Epoch 198/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0723\n",
      "Epoch 199/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0723\n",
      "Epoch 200/200\n",
      "146/146 [==============================] - 1s 7ms/step - loss: 0.0146 - acc: 0.0723\n",
      "Epoch 1/200\n",
      "9/9 [==============================] - 6s 617ms/step - loss: 0.1278 - acc: 0.0000e+00\n",
      "Epoch 2/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.1271 - acc: 0.1944\n",
      "Epoch 3/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1265 - acc: 0.3819\n",
      "Epoch 4/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.1258 - acc: 0.5069\n",
      "Epoch 5/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1251 - acc: 0.5556\n",
      "Epoch 6/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1244 - acc: 0.5556\n",
      "Epoch 7/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.1236 - acc: 0.5556\n",
      "Epoch 8/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1228 - acc: 0.5556\n",
      "Epoch 9/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.1219 - acc: 0.5556\n",
      "Epoch 10/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1209 - acc: 0.5556\n",
      "Epoch 11/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1199 - acc: 0.5556\n",
      "Epoch 12/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.1188 - acc: 0.5556\n",
      "Epoch 13/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1176 - acc: 0.5556\n",
      "Epoch 14/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1164 - acc: 0.5556\n",
      "Epoch 15/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1151 - acc: 0.5556\n",
      "Epoch 16/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.1137 - acc: 0.5556\n",
      "Epoch 17/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1122 - acc: 0.5556\n",
      "Epoch 18/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.1107 - acc: 0.5556\n",
      "Epoch 19/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1092 - acc: 0.5556\n",
      "Epoch 20/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.1076 - acc: 0.5556\n",
      "Epoch 21/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.1061 - acc: 0.5278\n",
      "Epoch 22/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1045 - acc: 0.4583\n",
      "Epoch 23/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1029 - acc: 0.4028\n",
      "Epoch 24/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.1014 - acc: 0.3333\n",
      "Epoch 25/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0999 - acc: 0.3264\n",
      "Epoch 26/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0983 - acc: 0.3125\n",
      "Epoch 27/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0968 - acc: 0.2986\n",
      "Epoch 28/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0953 - acc: 0.2778\n",
      "Epoch 29/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0938 - acc: 0.2569\n",
      "Epoch 30/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0923 - acc: 0.2292\n",
      "Epoch 31/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0909 - acc: 0.1806\n",
      "Epoch 32/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0894 - acc: 0.1667\n",
      "Epoch 33/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0879 - acc: 0.1319\n",
      "Epoch 34/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0865 - acc: 0.1111\n",
      "Epoch 35/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0851 - acc: 0.0903\n",
      "Epoch 36/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0838 - acc: 0.0694\n",
      "Epoch 37/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0825 - acc: 0.0556\n",
      "Epoch 38/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0812 - acc: 0.0556\n",
      "Epoch 39/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0800 - acc: 0.0139\n",
      "Epoch 40/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0788 - acc: 0.0000e+00\n",
      "Epoch 41/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0777 - acc: 0.0208\n",
      "Epoch 42/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0766 - acc: 0.0278\n",
      "Epoch 43/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0756 - acc: 0.0417\n",
      "Epoch 44/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0747 - acc: 0.0417\n",
      "Epoch 45/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0737 - acc: 0.0486\n",
      "Epoch 46/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0729 - acc: 0.0556\n",
      "Epoch 47/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0720 - acc: 0.0625\n",
      "Epoch 48/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0712 - acc: 0.0694\n",
      "Epoch 49/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0705 - acc: 0.0833\n",
      "Epoch 50/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0698 - acc: 0.0833\n",
      "Epoch 51/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0691 - acc: 0.0903\n",
      "Epoch 52/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0684 - acc: 0.0903\n",
      "Epoch 53/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0678 - acc: 0.0903\n",
      "Epoch 54/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0673 - acc: 0.0903\n",
      "Epoch 55/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0667 - acc: 0.0903\n",
      "Epoch 56/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0662 - acc: 0.0833\n",
      "Epoch 57/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0657 - acc: 0.0833\n",
      "Epoch 58/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0652 - acc: 0.0833\n",
      "Epoch 59/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0648 - acc: 0.0833\n",
      "Epoch 60/200\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0643 - acc: 0.0833\n",
      "Epoch 61/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0639 - acc: 0.0833\n",
      "Epoch 62/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0635 - acc: 0.0833\n",
      "Epoch 63/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0631 - acc: 0.0833\n",
      "Epoch 64/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0628 - acc: 0.0833\n",
      "Epoch 65/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0624 - acc: 0.0833\n",
      "Epoch 66/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0621 - acc: 0.0833\n",
      "Epoch 67/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0618 - acc: 0.0833\n",
      "Epoch 68/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0615 - acc: 0.0764\n",
      "Epoch 69/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0612 - acc: 0.0764\n",
      "Epoch 70/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0609 - acc: 0.0764\n",
      "Epoch 71/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0606 - acc: 0.0764\n",
      "Epoch 72/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0604 - acc: 0.0764\n",
      "Epoch 73/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0601 - acc: 0.0764\n",
      "Epoch 74/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0599 - acc: 0.0764\n",
      "Epoch 75/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0596 - acc: 0.0764\n",
      "Epoch 76/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0594 - acc: 0.0764\n",
      "Epoch 77/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0592 - acc: 0.0833\n",
      "Epoch 78/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0590 - acc: 0.0833\n",
      "Epoch 79/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0588 - acc: 0.0833\n",
      "Epoch 80/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0586 - acc: 0.0833\n",
      "Epoch 81/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0585 - acc: 0.0972\n",
      "Epoch 82/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0583 - acc: 0.0903\n",
      "Epoch 83/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0581 - acc: 0.0764\n",
      "Epoch 84/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0579 - acc: 0.0694\n",
      "Epoch 85/200\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0578 - acc: 0.0625\n",
      "Epoch 86/200\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0576 - acc: 0.0625\n",
      "Epoch 87/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0575 - acc: 0.0556\n",
      "Epoch 88/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0574 - acc: 0.0556\n",
      "Epoch 89/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0572 - acc: 0.0556\n",
      "Epoch 90/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0571 - acc: 0.0556\n",
      "Epoch 91/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0570 - acc: 0.0486\n",
      "Epoch 92/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0568 - acc: 0.0417\n",
      "Epoch 93/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0567 - acc: 0.0347\n",
      "Epoch 94/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0566 - acc: 0.0347\n",
      "Epoch 95/200\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.0565 - acc: 0.0347\n",
      "Epoch 96/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0564 - acc: 0.0347\n",
      "Epoch 97/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0562 - acc: 0.0347\n",
      "Epoch 98/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0561 - acc: 0.0347\n",
      "Epoch 99/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0560 - acc: 0.0347\n",
      "Epoch 100/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0559 - acc: 0.0347\n",
      "Epoch 101/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0558 - acc: 0.0347\n",
      "Epoch 102/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0557 - acc: 0.0347\n",
      "Epoch 103/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0556 - acc: 0.0347\n",
      "Epoch 104/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0555 - acc: 0.0347\n",
      "Epoch 105/200\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.0554 - acc: 0.0347\n",
      "Epoch 106/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0553 - acc: 0.0347\n",
      "Epoch 107/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0552 - acc: 0.0347\n",
      "Epoch 108/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0551 - acc: 0.0347\n",
      "Epoch 109/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0550 - acc: 0.0347\n",
      "Epoch 110/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0549 - acc: 0.0347\n",
      "Epoch 111/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0548 - acc: 0.0347\n",
      "Epoch 112/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0547 - acc: 0.0347\n",
      "Epoch 113/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0546 - acc: 0.0347\n",
      "Epoch 114/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0545 - acc: 0.0486\n",
      "Epoch 115/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0544 - acc: 0.0625\n",
      "Epoch 116/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0543 - acc: 0.0694\n",
      "Epoch 117/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0542 - acc: 0.1111\n",
      "Epoch 118/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0541 - acc: 0.1111\n",
      "Epoch 119/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0540 - acc: 0.1111\n",
      "Epoch 120/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0539 - acc: 0.1042\n",
      "Epoch 121/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0538 - acc: 0.1042\n",
      "Epoch 122/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0537 - acc: 0.1111\n",
      "Epoch 123/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0536 - acc: 0.1111\n",
      "Epoch 124/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0535 - acc: 0.1111\n",
      "Epoch 125/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0533 - acc: 0.1111\n",
      "Epoch 126/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0532 - acc: 0.1111\n",
      "Epoch 127/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0531 - acc: 0.1111\n",
      "Epoch 128/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0530 - acc: 0.1111\n",
      "Epoch 129/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0529 - acc: 0.1181\n",
      "Epoch 130/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0528 - acc: 0.1181\n",
      "Epoch 131/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0527 - acc: 0.1181\n",
      "Epoch 132/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0525 - acc: 0.1181\n",
      "Epoch 133/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0524 - acc: 0.1181\n",
      "Epoch 134/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0523 - acc: 0.1181\n",
      "Epoch 135/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0522 - acc: 0.1181\n",
      "Epoch 136/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0521 - acc: 0.1181\n",
      "Epoch 137/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0520 - acc: 0.1181\n",
      "Epoch 138/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0519 - acc: 0.1181\n",
      "Epoch 139/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0518 - acc: 0.1181\n",
      "Epoch 140/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0517 - acc: 0.1319\n",
      "Epoch 141/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0516 - acc: 0.1319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0515 - acc: 0.1389\n",
      "Epoch 143/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0514 - acc: 0.1597\n",
      "Epoch 144/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0513 - acc: 0.1597\n",
      "Epoch 145/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0512 - acc: 0.1597\n",
      "Epoch 146/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0511 - acc: 0.1597\n",
      "Epoch 147/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0510 - acc: 0.1597\n",
      "Epoch 148/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0509 - acc: 0.1806\n",
      "Epoch 149/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0507 - acc: 0.1875\n",
      "Epoch 150/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0506 - acc: 0.1875\n",
      "Epoch 151/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0505 - acc: 0.1875\n",
      "Epoch 152/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0504 - acc: 0.1875\n",
      "Epoch 153/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0503 - acc: 0.1875\n",
      "Epoch 154/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0502 - acc: 0.1875\n",
      "Epoch 155/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0501 - acc: 0.1875\n",
      "Epoch 156/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0500 - acc: 0.1875\n",
      "Epoch 157/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0499 - acc: 0.1875\n",
      "Epoch 158/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0498 - acc: 0.1875\n",
      "Epoch 159/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0497 - acc: 0.1875\n",
      "Epoch 160/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0496 - acc: 0.2222\n",
      "Epoch 161/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0495 - acc: 0.2222\n",
      "Epoch 162/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0494 - acc: 0.2222\n",
      "Epoch 163/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0493 - acc: 0.2222\n",
      "Epoch 164/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0492 - acc: 0.2222\n",
      "Epoch 165/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0491 - acc: 0.2222\n",
      "Epoch 166/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0490 - acc: 0.2222\n",
      "Epoch 167/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0489 - acc: 0.2222\n",
      "Epoch 168/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0488 - acc: 0.2222\n",
      "Epoch 169/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0487 - acc: 0.2222\n",
      "Epoch 170/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0486 - acc: 0.2222\n",
      "Epoch 171/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0485 - acc: 0.2222\n",
      "Epoch 172/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0485 - acc: 0.2222\n",
      "Epoch 173/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0484 - acc: 0.2222\n",
      "Epoch 174/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0483 - acc: 0.2222\n",
      "Epoch 175/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0482 - acc: 0.2222\n",
      "Epoch 176/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0482 - acc: 0.2292\n",
      "Epoch 177/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0481 - acc: 0.2292\n",
      "Epoch 178/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0480 - acc: 0.2292\n",
      "Epoch 179/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0480 - acc: 0.2292\n",
      "Epoch 180/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0479 - acc: 0.2361\n",
      "Epoch 181/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0478 - acc: 0.2361\n",
      "Epoch 182/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0477 - acc: 0.2431\n",
      "Epoch 183/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0477 - acc: 0.2431\n",
      "Epoch 184/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0476 - acc: 0.2500\n",
      "Epoch 185/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0475 - acc: 0.2500\n",
      "Epoch 186/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0475 - acc: 0.2500\n",
      "Epoch 187/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0474 - acc: 0.2500\n",
      "Epoch 188/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0473 - acc: 0.2500\n",
      "Epoch 189/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0473 - acc: 0.2569\n",
      "Epoch 190/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0472 - acc: 0.2639\n",
      "Epoch 191/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0472 - acc: 0.2639\n",
      "Epoch 192/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0471 - acc: 0.2639\n",
      "Epoch 193/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0470 - acc: 0.2639\n",
      "Epoch 194/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0470 - acc: 0.2639\n",
      "Epoch 195/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0469 - acc: 0.2639\n",
      "Epoch 196/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0469 - acc: 0.2639\n",
      "Epoch 197/200\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.0468 - acc: 0.2639\n",
      "Epoch 198/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0468 - acc: 0.2639\n",
      "Epoch 199/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0467 - acc: 0.2639\n",
      "Epoch 200/200\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.0467 - acc: 0.2639\n",
      "Epoch 1/200\n",
      "21/21 [==============================] - 6s 284ms/step - loss: 0.1679 - acc: 0.1905\n",
      "Epoch 2/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.1646 - acc: 0.1042\n",
      "Epoch 3/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.1608 - acc: 0.0804\n",
      "Epoch 4/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.1561 - acc: 0.0804\n",
      "Epoch 5/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.1505 - acc: 0.0804\n",
      "Epoch 6/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.1438 - acc: 0.0804\n",
      "Epoch 7/200\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 0.1365 - acc: 0.0804\n",
      "Epoch 8/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.1289 - acc: 0.0446\n",
      "Epoch 9/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.1216 - acc: 0.0327\n",
      "Epoch 10/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.1147 - acc: 0.0208\n",
      "Epoch 11/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.1082 - acc: 0.0179\n",
      "Epoch 12/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.1023 - acc: 0.0149\n",
      "Epoch 13/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0970 - acc: 0.0149\n",
      "Epoch 14/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0923 - acc: 0.0149\n",
      "Epoch 15/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0882 - acc: 0.0119\n",
      "Epoch 16/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0848 - acc: 0.0119\n",
      "Epoch 17/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0817 - acc: 0.0119\n",
      "Epoch 18/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0791 - acc: 0.0089\n",
      "Epoch 19/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0767 - acc: 0.0089\n",
      "Epoch 20/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0745 - acc: 0.0089\n",
      "Epoch 21/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0726 - acc: 0.0089\n",
      "Epoch 22/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0708 - acc: 0.0089\n",
      "Epoch 23/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0691 - acc: 0.0060\n",
      "Epoch 24/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0677 - acc: 0.0060\n",
      "Epoch 25/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0663 - acc: 0.0060\n",
      "Epoch 26/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0651 - acc: 0.0060\n",
      "Epoch 27/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0640 - acc: 0.0060\n",
      "Epoch 28/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0629 - acc: 0.0030\n",
      "Epoch 29/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0618 - acc: 0.0030\n",
      "Epoch 30/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0608 - acc: 0.0030\n",
      "Epoch 31/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.0599 - acc: 0.0030\n",
      "Epoch 32/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0590 - acc: 0.0030\n",
      "Epoch 33/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.0582 - acc: 0.0030\n",
      "Epoch 34/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.0574 - acc: 0.0030\n",
      "Epoch 35/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0566 - acc: 0.0030\n",
      "Epoch 36/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0559 - acc: 0.0030\n",
      "Epoch 37/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0552 - acc: 0.0030\n",
      "Epoch 38/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0546 - acc: 0.0030\n",
      "Epoch 39/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0540 - acc: 0.0030\n",
      "Epoch 40/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.0534 - acc: 0.0030\n",
      "Epoch 41/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0529 - acc: 0.0000e+00\n",
      "Epoch 42/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0524 - acc: 0.0000e+00\n",
      "Epoch 43/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0520 - acc: 0.0000e+00\n",
      "Epoch 44/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0516 - acc: 0.0000e+00\n",
      "Epoch 45/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0512 - acc: 0.0000e+00\n",
      "Epoch 46/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.0508 - acc: 0.0000e+00\n",
      "Epoch 47/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0504 - acc: 0.0030\n",
      "Epoch 48/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0501 - acc: 0.0060\n",
      "Epoch 49/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0498 - acc: 0.0119\n",
      "Epoch 50/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0495 - acc: 0.0179\n",
      "Epoch 51/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0492 - acc: 0.0208\n",
      "Epoch 52/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0489 - acc: 0.0208\n",
      "Epoch 53/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0486 - acc: 0.0208\n",
      "Epoch 54/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0483 - acc: 0.0179\n",
      "Epoch 55/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0481 - acc: 0.0119\n",
      "Epoch 56/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0478 - acc: 0.0000e+00\n",
      "Epoch 57/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0476 - acc: 0.0000e+00\n",
      "Epoch 58/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0473 - acc: 0.0000e+00\n",
      "Epoch 59/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0471 - acc: 0.0000e+00\n",
      "Epoch 60/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0469 - acc: 0.0000e+00\n",
      "Epoch 61/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0467 - acc: 0.0000e+00\n",
      "Epoch 62/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0465 - acc: 0.0000e+00\n",
      "Epoch 63/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0463 - acc: 0.0000e+00\n",
      "Epoch 64/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0461 - acc: 0.0000e+00\n",
      "Epoch 65/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0459 - acc: 0.0000e+00\n",
      "Epoch 66/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0457 - acc: 0.0000e+00\n",
      "Epoch 67/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0455 - acc: 0.0000e+00\n",
      "Epoch 68/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0453 - acc: 0.0000e+00\n",
      "Epoch 69/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0451 - acc: 0.0000e+00\n",
      "Epoch 70/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0449 - acc: 0.0000e+00\n",
      "Epoch 71/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0447 - acc: 0.0000e+00\n",
      "Epoch 72/200\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 0.0446 - acc: 0.0000e+00\n",
      "Epoch 73/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0444 - acc: 0.0000e+00\n",
      "Epoch 74/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0442 - acc: 0.0000e+00\n",
      "Epoch 75/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0441 - acc: 0.0000e+00\n",
      "Epoch 76/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0439 - acc: 0.0000e+00\n",
      "Epoch 77/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0437 - acc: 0.0000e+00\n",
      "Epoch 78/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0436 - acc: 0.0000e+00\n",
      "Epoch 79/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0434 - acc: 0.0000e+00\n",
      "Epoch 80/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0433 - acc: 0.0000e+00\n",
      "Epoch 81/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0431 - acc: 0.0000e+00\n",
      "Epoch 82/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0430 - acc: 0.0000e+00\n",
      "Epoch 83/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0428 - acc: 0.0000e+00\n",
      "Epoch 84/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0427 - acc: 0.0000e+00\n",
      "Epoch 85/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0426 - acc: 0.0000e+00\n",
      "Epoch 86/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0424 - acc: 0.0000e+00\n",
      "Epoch 87/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0423 - acc: 0.0000e+00\n",
      "Epoch 88/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0422 - acc: 0.0000e+00\n",
      "Epoch 89/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0420 - acc: 0.0000e+00\n",
      "Epoch 90/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0419 - acc: 0.0000e+00\n",
      "Epoch 91/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0418 - acc: 0.0000e+00\n",
      "Epoch 92/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0417 - acc: 0.0000e+00\n",
      "Epoch 93/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0416 - acc: 0.0000e+00\n",
      "Epoch 94/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0415 - acc: 0.0000e+00\n",
      "Epoch 95/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.0414 - acc: 0.0000e+00\n",
      "Epoch 96/200\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 0.0413 - acc: 0.0000e+00\n",
      "Epoch 97/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.0412 - acc: 0.0000e+00\n",
      "Epoch 98/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0411 - acc: 0.0000e+00\n",
      "Epoch 99/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0410 - acc: 0.0000e+00\n",
      "Epoch 100/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0409 - acc: 0.0000e+00\n",
      "Epoch 101/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0408 - acc: 0.0000e+00\n",
      "Epoch 102/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0408 - acc: 0.0119\n",
      "Epoch 103/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0407 - acc: 0.0268\n",
      "Epoch 104/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.0406 - acc: 0.0268\n",
      "Epoch 105/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0405 - acc: 0.0268\n",
      "Epoch 106/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0405 - acc: 0.0268\n",
      "Epoch 107/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0404 - acc: 0.0268\n",
      "Epoch 108/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0403 - acc: 0.0268\n",
      "Epoch 109/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0402 - acc: 0.0268\n",
      "Epoch 110/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0402 - acc: 0.0268\n",
      "Epoch 111/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0401 - acc: 0.0268\n",
      "Epoch 112/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0400 - acc: 0.0268\n",
      "Epoch 113/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0400 - acc: 0.0268\n",
      "Epoch 114/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0399 - acc: 0.0268\n",
      "Epoch 115/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0399 - acc: 0.0268\n",
      "Epoch 116/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0398 - acc: 0.0268\n",
      "Epoch 117/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0397 - acc: 0.0268\n",
      "Epoch 118/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0397 - acc: 0.0268\n",
      "Epoch 119/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0396 - acc: 0.0268\n",
      "Epoch 120/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0396 - acc: 0.0268\n",
      "Epoch 121/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0395 - acc: 0.0268\n",
      "Epoch 122/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0395 - acc: 0.0268\n",
      "Epoch 123/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0394 - acc: 0.0268\n",
      "Epoch 124/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0394 - acc: 0.0268\n",
      "Epoch 125/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0393 - acc: 0.0268\n",
      "Epoch 126/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0392 - acc: 0.0268\n",
      "Epoch 127/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0392 - acc: 0.0268\n",
      "Epoch 128/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0391 - acc: 0.0268\n",
      "Epoch 129/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0391 - acc: 0.0268\n",
      "Epoch 130/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0390 - acc: 0.0268\n",
      "Epoch 131/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0390 - acc: 0.0268\n",
      "Epoch 132/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0389 - acc: 0.0268\n",
      "Epoch 133/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0389 - acc: 0.0268\n",
      "Epoch 134/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0388 - acc: 0.0268\n",
      "Epoch 135/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0388 - acc: 0.0268\n",
      "Epoch 136/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0388 - acc: 0.0268\n",
      "Epoch 137/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0387 - acc: 0.0268\n",
      "Epoch 138/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0387 - acc: 0.0268\n",
      "Epoch 139/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0386 - acc: 0.0268\n",
      "Epoch 140/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0386 - acc: 0.0268\n",
      "Epoch 141/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0385 - acc: 0.0268\n",
      "Epoch 142/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0385 - acc: 0.0268\n",
      "Epoch 143/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0385 - acc: 0.0268\n",
      "Epoch 144/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0384 - acc: 0.0268\n",
      "Epoch 145/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0384 - acc: 0.0268\n",
      "Epoch 146/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0383 - acc: 0.0268\n",
      "Epoch 147/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0383 - acc: 0.0268\n",
      "Epoch 148/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0383 - acc: 0.0268\n",
      "Epoch 149/200\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 0.0382 - acc: 0.0268\n",
      "Epoch 150/200\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 0.0382 - acc: 0.0268\n",
      "Epoch 151/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.0381 - acc: 0.0268\n",
      "Epoch 152/200\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 0.0381 - acc: 0.0268\n",
      "Epoch 153/200\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 0.0381 - acc: 0.0268\n",
      "Epoch 154/200\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 0.0380 - acc: 0.0268\n",
      "Epoch 155/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.0380 - acc: 0.0268\n",
      "Epoch 156/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.0380 - acc: 0.0268\n",
      "Epoch 157/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0379 - acc: 0.0268\n",
      "Epoch 158/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0379 - acc: 0.0268\n",
      "Epoch 159/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.0378 - acc: 0.0268\n",
      "Epoch 160/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.0378 - acc: 0.0268\n",
      "Epoch 161/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0378 - acc: 0.0268\n",
      "Epoch 162/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0377 - acc: 0.0268\n",
      "Epoch 163/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0377 - acc: 0.0268\n",
      "Epoch 164/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0377 - acc: 0.0268\n",
      "Epoch 165/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0376 - acc: 0.0268\n",
      "Epoch 166/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0376 - acc: 0.0268\n",
      "Epoch 167/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0376 - acc: 0.0268\n",
      "Epoch 168/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0376 - acc: 0.0268\n",
      "Epoch 169/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0375 - acc: 0.0268\n",
      "Epoch 170/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0375 - acc: 0.0268\n",
      "Epoch 171/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0375 - acc: 0.0268\n",
      "Epoch 172/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0374 - acc: 0.0268\n",
      "Epoch 173/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0374 - acc: 0.0268\n",
      "Epoch 174/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0374 - acc: 0.0268\n",
      "Epoch 175/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0374 - acc: 0.0268\n",
      "Epoch 176/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0373 - acc: 0.0268\n",
      "Epoch 177/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0373 - acc: 0.0268\n",
      "Epoch 178/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0373 - acc: 0.0268\n",
      "Epoch 179/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0372 - acc: 0.0268\n",
      "Epoch 180/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.0372 - acc: 0.0268\n",
      "Epoch 181/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0372 - acc: 0.0268\n",
      "Epoch 182/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0372 - acc: 0.0268\n",
      "Epoch 183/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.0371 - acc: 0.0268\n",
      "Epoch 184/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0371 - acc: 0.0268\n",
      "Epoch 185/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.0371 - acc: 0.0268\n",
      "Epoch 186/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0371 - acc: 0.0268\n",
      "Epoch 187/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.0371 - acc: 0.0268\n",
      "Epoch 188/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.0370 - acc: 0.0268\n",
      "Epoch 189/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.0370 - acc: 0.0268\n",
      "Epoch 190/200\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 0.0370 - acc: 0.0268\n",
      "Epoch 191/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.0370 - acc: 0.0268\n",
      "Epoch 192/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0370 - acc: 0.0268\n",
      "Epoch 193/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.0369 - acc: 0.0268\n",
      "Epoch 194/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0369 - acc: 0.0268\n",
      "Epoch 195/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0369 - acc: 0.0268\n",
      "Epoch 196/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0369 - acc: 0.0268\n",
      "Epoch 197/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0369 - acc: 0.0268\n",
      "Epoch 198/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0368 - acc: 0.0268\n",
      "Epoch 199/200\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.0368 - acc: 0.0268\n",
      "Epoch 200/200\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.0368 - acc: 0.0268\n",
      "Epoch 1/200\n",
      "138/138 [==============================] - 7s 51ms/step - loss: 0.2455 - acc: 4.5290e-04\n",
      "Epoch 2/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.2008 - acc: 0.0000e+00\n",
      "Epoch 3/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.1411 - acc: 0.0000e+00\n",
      "Epoch 4/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0950 - acc: 0.1544\n",
      "Epoch 5/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0711 - acc: 0.4841\n",
      "Epoch 6/200\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 0.0575 - acc: 0.5924\n",
      "Epoch 7/200\n",
      "138/138 [==============================] - 2s 13ms/step - loss: 0.0490 - acc: 0.6381\n",
      "Epoch 8/200\n",
      "138/138 [==============================] - 1s 11ms/step - loss: 0.0433 - acc: 0.6513\n",
      "Epoch 9/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0393 - acc: 0.2894\n",
      "Epoch 10/200\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0361 - acc: 0.2151\n",
      "Epoch 11/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0336 - acc: 0.2355\n",
      "Epoch 12/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0314 - acc: 0.2745\n",
      "Epoch 13/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0294 - acc: 0.2930\n",
      "Epoch 14/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0276 - acc: 0.3297\n",
      "Epoch 15/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0259 - acc: 0.3379\n",
      "Epoch 16/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0246 - acc: 0.3370\n",
      "Epoch 17/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0235 - acc: 0.3465\n",
      "Epoch 18/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0225 - acc: 0.3614\n",
      "Epoch 19/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0217 - acc: 0.3931\n",
      "Epoch 20/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0210 - acc: 0.3918\n",
      "Epoch 21/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0205 - acc: 0.3936\n",
      "Epoch 22/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0200 - acc: 0.4004\n",
      "Epoch 23/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0196 - acc: 0.3976\n",
      "Epoch 24/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0192 - acc: 0.3967\n",
      "Epoch 25/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0189 - acc: 0.3936\n",
      "Epoch 26/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0186 - acc: 0.3954\n",
      "Epoch 27/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0183 - acc: 0.3986\n",
      "Epoch 28/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0181 - acc: 0.4257\n",
      "Epoch 29/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0179 - acc: 0.4620\n",
      "Epoch 30/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0178 - acc: 0.4606\n",
      "Epoch 31/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0176 - acc: 0.4611\n",
      "Epoch 32/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0175 - acc: 0.4583\n",
      "Epoch 33/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0174 - acc: 0.4615\n",
      "Epoch 34/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0174 - acc: 0.4651\n",
      "Epoch 35/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0173 - acc: 0.4620\n",
      "Epoch 36/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0172 - acc: 0.4687\n",
      "Epoch 37/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0171 - acc: 0.4642\n",
      "Epoch 38/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0171 - acc: 0.4592\n",
      "Epoch 39/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0170 - acc: 0.4701\n",
      "Epoch 40/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0170 - acc: 0.4719\n",
      "Epoch 41/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0169 - acc: 0.4339\n",
      "Epoch 42/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0169 - acc: 0.4158\n",
      "Epoch 43/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0168 - acc: 0.4262\n",
      "Epoch 44/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0168 - acc: 0.4212\n",
      "Epoch 45/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0168 - acc: 0.4185\n",
      "Epoch 46/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0167 - acc: 0.4216\n",
      "Epoch 47/200\n",
      "138/138 [==============================] - 1s 6ms/step - loss: 0.0167 - acc: 0.4158\n",
      "Epoch 48/200\n",
      "138/138 [==============================] - 1s 6ms/step - loss: 0.0167 - acc: 0.4248\n",
      "Epoch 49/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0166 - acc: 0.4167\n",
      "Epoch 50/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0166 - acc: 0.4203\n",
      "Epoch 51/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0166 - acc: 0.4216\n",
      "Epoch 52/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0166 - acc: 0.4257\n",
      "Epoch 53/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0165 - acc: 0.4271\n",
      "Epoch 54/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0165 - acc: 0.4257\n",
      "Epoch 55/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0165 - acc: 0.4244\n",
      "Epoch 56/200\n",
      "138/138 [==============================] - 1s 6ms/step - loss: 0.0165 - acc: 0.4271\n",
      "Epoch 57/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0164 - acc: 0.4298\n",
      "Epoch 58/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0164 - acc: 0.4248\n",
      "Epoch 59/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0164 - acc: 0.4244\n",
      "Epoch 60/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0164 - acc: 0.4321\n",
      "Epoch 61/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.4221\n",
      "Epoch 62/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.4262\n",
      "Epoch 63/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.4221\n",
      "Epoch 64/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.4293\n",
      "Epoch 65/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.4334\n",
      "Epoch 66/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0162 - acc: 0.4348\n",
      "Epoch 67/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0162 - acc: 0.4257\n",
      "Epoch 68/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0162 - acc: 0.4312\n",
      "Epoch 69/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0162 - acc: 0.4303\n",
      "Epoch 70/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0162 - acc: 0.4366\n",
      "Epoch 71/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0162 - acc: 0.4380\n",
      "Epoch 72/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0162 - acc: 0.4312\n",
      "Epoch 73/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0162 - acc: 0.4357\n",
      "Epoch 74/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0161 - acc: 0.4321\n",
      "Epoch 75/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0161 - acc: 0.4407\n",
      "Epoch 76/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0161 - acc: 0.4239\n",
      "Epoch 77/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0161 - acc: 0.4262\n",
      "Epoch 78/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0161 - acc: 0.4321\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0161 - acc: 0.4370\n",
      "Epoch 80/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0161 - acc: 0.4307\n",
      "Epoch 81/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0161 - acc: 0.4384\n",
      "Epoch 82/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0161 - acc: 0.4275\n",
      "Epoch 83/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0160 - acc: 0.4425\n",
      "Epoch 84/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0160 - acc: 0.4343\n",
      "Epoch 85/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0160 - acc: 0.4398\n",
      "Epoch 86/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0160 - acc: 0.4343\n",
      "Epoch 87/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0160 - acc: 0.4348\n",
      "Epoch 88/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0160 - acc: 0.4325\n",
      "Epoch 89/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0160 - acc: 0.4343\n",
      "Epoch 90/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0160 - acc: 0.4298\n",
      "Epoch 91/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0160 - acc: 0.4384\n",
      "Epoch 92/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0160 - acc: 0.4398\n",
      "Epoch 93/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0160 - acc: 0.4411\n",
      "Epoch 94/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0160 - acc: 0.4375\n",
      "Epoch 95/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0159 - acc: 0.4429\n",
      "Epoch 96/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0159 - acc: 0.4443\n",
      "Epoch 97/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0159 - acc: 0.4398\n",
      "Epoch 98/200\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0159 - acc: 0.4420\n",
      "Epoch 99/200\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0159 - acc: 0.4475\n",
      "Epoch 100/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0159 - acc: 0.4484\n",
      "Epoch 101/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0159 - acc: 0.4561\n",
      "Epoch 102/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0159 - acc: 0.4502\n",
      "Epoch 103/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0159 - acc: 0.4366\n",
      "Epoch 104/200\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0159 - acc: 0.4547\n",
      "Epoch 105/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0159 - acc: 0.4457\n",
      "Epoch 106/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0159 - acc: 0.4479\n",
      "Epoch 107/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0159 - acc: 0.4547\n",
      "Epoch 108/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0159 - acc: 0.4484\n",
      "Epoch 109/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0158 - acc: 0.4520\n",
      "Epoch 110/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0158 - acc: 0.4588\n",
      "Epoch 111/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0158 - acc: 0.4524\n",
      "Epoch 112/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0158 - acc: 0.4420\n",
      "Epoch 113/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0158 - acc: 0.4565\n",
      "Epoch 114/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0158 - acc: 0.4502\n",
      "Epoch 115/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0158 - acc: 0.4565\n",
      "Epoch 116/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0158 - acc: 0.4620\n",
      "Epoch 117/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0158 - acc: 0.4538\n",
      "Epoch 118/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0158 - acc: 0.4561\n",
      "Epoch 119/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0158 - acc: 0.4524\n",
      "Epoch 120/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0158 - acc: 0.4570\n",
      "Epoch 121/200\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0158 - acc: 0.4606\n",
      "Epoch 122/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0158 - acc: 0.4493\n",
      "Epoch 123/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0158 - acc: 0.4592\n",
      "Epoch 124/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0158 - acc: 0.4488\n",
      "Epoch 125/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0158 - acc: 0.4461\n",
      "Epoch 126/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0158 - acc: 0.4615\n",
      "Epoch 127/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0158 - acc: 0.4493\n",
      "Epoch 128/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0158 - acc: 0.4529\n",
      "Epoch 129/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0158 - acc: 0.4574\n",
      "Epoch 130/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0158 - acc: 0.4443\n",
      "Epoch 131/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0157 - acc: 0.4497\n",
      "Epoch 132/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0157 - acc: 0.4547\n",
      "Epoch 133/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0157 - acc: 0.4515\n",
      "Epoch 134/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0157 - acc: 0.4520\n",
      "Epoch 135/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0157 - acc: 0.4556\n",
      "Epoch 136/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0157 - acc: 0.4529\n",
      "Epoch 137/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0157 - acc: 0.4461\n",
      "Epoch 138/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0157 - acc: 0.4601\n",
      "Epoch 139/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0157 - acc: 0.4669\n",
      "Epoch 140/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0157 - acc: 0.4683\n",
      "Epoch 141/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0157 - acc: 0.4574\n",
      "Epoch 142/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0157 - acc: 0.4574\n",
      "Epoch 143/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0157 - acc: 0.4556\n",
      "Epoch 144/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0157 - acc: 0.4588\n",
      "Epoch 145/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0157 - acc: 0.4543\n",
      "Epoch 146/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0157 - acc: 0.4570\n",
      "Epoch 147/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0157 - acc: 0.4633\n",
      "Epoch 148/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0157 - acc: 0.4583\n",
      "Epoch 149/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0157 - acc: 0.4538\n",
      "Epoch 150/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0157 - acc: 0.4579\n",
      "Epoch 151/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0157 - acc: 0.4588\n",
      "Epoch 152/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0157 - acc: 0.4624\n",
      "Epoch 153/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0157 - acc: 0.4642\n",
      "Epoch 154/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0157 - acc: 0.4524\n",
      "Epoch 155/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0157 - acc: 0.4529\n",
      "Epoch 156/200\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0157 - acc: 0.4565\n",
      "Epoch 157/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0157 - acc: 0.4746\n",
      "Epoch 158/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0157 - acc: 0.4579\n",
      "Epoch 159/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0157 - acc: 0.4647\n",
      "Epoch 160/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0157 - acc: 0.4552\n",
      "Epoch 161/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0157 - acc: 0.4592\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0157 - acc: 0.4579\n",
      "Epoch 163/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0157 - acc: 0.4606\n",
      "Epoch 164/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0157 - acc: 0.4592\n",
      "Epoch 165/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0157 - acc: 0.4633\n",
      "Epoch 166/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4588\n",
      "Epoch 167/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4574\n",
      "Epoch 168/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0156 - acc: 0.4615\n",
      "Epoch 169/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0156 - acc: 0.4701\n",
      "Epoch 170/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0156 - acc: 0.4552\n",
      "Epoch 171/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0156 - acc: 0.4597\n",
      "Epoch 172/200\n",
      "138/138 [==============================] - 2s 11ms/step - loss: 0.0156 - acc: 0.4665\n",
      "Epoch 173/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0156 - acc: 0.4606\n",
      "Epoch 174/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4633\n",
      "Epoch 175/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0156 - acc: 0.4638\n",
      "Epoch 176/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0156 - acc: 0.4579\n",
      "Epoch 177/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4565\n",
      "Epoch 178/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4629\n",
      "Epoch 179/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4565\n",
      "Epoch 180/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4611\n",
      "Epoch 181/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4706\n",
      "Epoch 182/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4570\n",
      "Epoch 183/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4597\n",
      "Epoch 184/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4547\n",
      "Epoch 185/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4620\n",
      "Epoch 186/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4651\n",
      "Epoch 187/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4660\n",
      "Epoch 188/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4678\n",
      "Epoch 189/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4687\n",
      "Epoch 190/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4701\n",
      "Epoch 191/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4647\n",
      "Epoch 192/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4629\n",
      "Epoch 193/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4706\n",
      "Epoch 194/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4570\n",
      "Epoch 195/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4683\n",
      "Epoch 196/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4660\n",
      "Epoch 197/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4665\n",
      "Epoch 198/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4706\n",
      "Epoch 199/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4742\n",
      "Epoch 200/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0156 - acc: 0.4583\n",
      "Epoch 1/200\n",
      "138/138 [==============================] - 8s 60ms/step - loss: 0.2419 - acc: 0.1884\n",
      "Epoch 2/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.2086 - acc: 0.1395\n",
      "Epoch 3/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.1544 - acc: 0.0000e+00\n",
      "Epoch 4/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.1076 - acc: 0.0000e+00\n",
      "Epoch 5/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0790 - acc: 0.0000e+00A: 1s - loss: 0.0966 - acc\n",
      "Epoch 6/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0607 - acc: 0.0095\n",
      "Epoch 7/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0495 - acc: 0.0063\n",
      "Epoch 8/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0431 - acc: 0.0018\n",
      "Epoch 9/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0392 - acc: 0.0177\n",
      "Epoch 10/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0364 - acc: 0.0177\n",
      "Epoch 11/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0342 - acc: 0.0172\n",
      "Epoch 12/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0323 - acc: 0.0159\n",
      "Epoch 13/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0306 - acc: 0.0159\n",
      "Epoch 14/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0290 - acc: 0.0149\n",
      "Epoch 15/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0275 - acc: 0.0159\n",
      "Epoch 16/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0262 - acc: 0.0159\n",
      "Epoch 17/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0252 - acc: 0.0154\n",
      "Epoch 18/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0245 - acc: 0.0168\n",
      "Epoch 19/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0240 - acc: 0.0163\n",
      "Epoch 20/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0236 - acc: 0.0163\n",
      "Epoch 21/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0232 - acc: 0.0159\n",
      "Epoch 22/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0228 - acc: 0.0159\n",
      "Epoch 23/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0225 - acc: 0.0163\n",
      "Epoch 24/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0222 - acc: 0.0159\n",
      "Epoch 25/200\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0219 - acc: 0.0159\n",
      "Epoch 26/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0216 - acc: 0.0159\n",
      "Epoch 27/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0213 - acc: 0.0159\n",
      "Epoch 28/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0211 - acc: 0.0163\n",
      "Epoch 29/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0208 - acc: 0.0159\n",
      "Epoch 30/200\n",
      "138/138 [==============================] - 1s 6ms/step - loss: 0.0206 - acc: 0.0159\n",
      "Epoch 31/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0204 - acc: 0.0159\n",
      "Epoch 32/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0202 - acc: 0.0159\n",
      "Epoch 33/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0201 - acc: 0.0154\n",
      "Epoch 34/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0200 - acc: 0.0159\n",
      "Epoch 35/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0199 - acc: 0.0163\n",
      "Epoch 36/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0198 - acc: 0.0159\n",
      "Epoch 37/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0197 - acc: 0.0159\n",
      "Epoch 38/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0196 - acc: 0.0163\n",
      "Epoch 39/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0196 - acc: 0.0159\n",
      "Epoch 40/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0195 - acc: 0.0159\n",
      "Epoch 41/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0194 - acc: 0.0163\n",
      "Epoch 42/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0193 - acc: 0.0163\n",
      "Epoch 43/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0193 - acc: 0.0163\n",
      "Epoch 44/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0192 - acc: 0.0159\n",
      "Epoch 45/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0192 - acc: 0.0154\n",
      "Epoch 46/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0191 - acc: 0.0154\n",
      "Epoch 47/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0191 - acc: 0.0159\n",
      "Epoch 48/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0190 - acc: 0.0163\n",
      "Epoch 49/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0190 - acc: 0.0159\n",
      "Epoch 50/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0190 - acc: 0.0163\n",
      "Epoch 51/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0189 - acc: 0.0159\n",
      "Epoch 52/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0189 - acc: 0.0163\n",
      "Epoch 53/200\n",
      "138/138 [==============================] - 2s 12ms/step - loss: 0.0189 - acc: 0.0159\n",
      "Epoch 54/200\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0188 - acc: 0.0163\n",
      "Epoch 55/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0188 - acc: 0.0159\n",
      "Epoch 56/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0187 - acc: 0.0159\n",
      "Epoch 57/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0187 - acc: 0.0159\n",
      "Epoch 58/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0187 - acc: 0.0159\n",
      "Epoch 59/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0186 - acc: 0.0159\n",
      "Epoch 60/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0186 - acc: 0.0163\n",
      "Epoch 61/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0186 - acc: 0.0159\n",
      "Epoch 62/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0185 - acc: 0.0154\n",
      "Epoch 63/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0185 - acc: 0.0168\n",
      "Epoch 64/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0184 - acc: 0.0168\n",
      "Epoch 65/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0184 - acc: 0.0163\n",
      "Epoch 66/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0184 - acc: 0.0154\n",
      "Epoch 67/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0183 - acc: 0.0154\n",
      "Epoch 68/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0183 - acc: 0.0163\n",
      "Epoch 69/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0183 - acc: 0.0163\n",
      "Epoch 70/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0182 - acc: 0.0168\n",
      "Epoch 71/200\n",
      "138/138 [==============================] - 1s 10ms/step - loss: 0.0182 - acc: 0.0154\n",
      "Epoch 72/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0182 - acc: 0.0168\n",
      "Epoch 73/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0181 - acc: 0.0159\n",
      "Epoch 74/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0181 - acc: 0.0163\n",
      "Epoch 75/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0180 - acc: 0.0159\n",
      "Epoch 76/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0180 - acc: 0.0163\n",
      "Epoch 77/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0180 - acc: 0.0163\n",
      "Epoch 78/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0179 - acc: 0.0168\n",
      "Epoch 79/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0179 - acc: 0.0163\n",
      "Epoch 80/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0178 - acc: 0.0159\n",
      "Epoch 81/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0178 - acc: 0.0168\n",
      "Epoch 82/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0178 - acc: 0.0159\n",
      "Epoch 83/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0177 - acc: 0.0163\n",
      "Epoch 84/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0177 - acc: 0.0163\n",
      "Epoch 85/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0176 - acc: 0.0163\n",
      "Epoch 86/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0176 - acc: 0.0159\n",
      "Epoch 87/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0176 - acc: 0.0168\n",
      "Epoch 88/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0175 - acc: 0.0163\n",
      "Epoch 89/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0175 - acc: 0.0159\n",
      "Epoch 90/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0174 - acc: 0.0154\n",
      "Epoch 91/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0174 - acc: 0.0159\n",
      "Epoch 92/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0174 - acc: 0.0154\n",
      "Epoch 93/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0173 - acc: 0.0163\n",
      "Epoch 94/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0173 - acc: 0.0163\n",
      "Epoch 95/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0173 - acc: 0.0163\n",
      "Epoch 96/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0172 - acc: 0.0154\n",
      "Epoch 97/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0172 - acc: 0.0168\n",
      "Epoch 98/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0172 - acc: 0.0168\n",
      "Epoch 99/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0171 - acc: 0.0168\n",
      "Epoch 100/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0171 - acc: 0.0159\n",
      "Epoch 101/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0171 - acc: 0.0163A: 0s - loss: 0.0181 - acc: 0.0\n",
      "Epoch 102/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0171 - acc: 0.0163\n",
      "Epoch 103/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0170 - acc: 0.0168\n",
      "Epoch 104/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0170 - acc: 0.0159\n",
      "Epoch 105/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0170 - acc: 0.0159\n",
      "Epoch 106/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0170 - acc: 0.0168\n",
      "Epoch 107/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0170 - acc: 0.0159\n",
      "Epoch 108/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0170 - acc: 0.0163\n",
      "Epoch 109/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0169 - acc: 0.0163\n",
      "Epoch 110/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0169 - acc: 0.0163\n",
      "Epoch 111/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0169 - acc: 0.0168\n",
      "Epoch 112/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0169 - acc: 0.0168\n",
      "Epoch 113/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0169 - acc: 0.0163\n",
      "Epoch 114/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0169 - acc: 0.0159\n",
      "Epoch 115/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0168 - acc: 0.0163\n",
      "Epoch 116/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0168 - acc: 0.0163\n",
      "Epoch 117/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0168 - acc: 0.0159\n",
      "Epoch 118/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0168 - acc: 0.0163\n",
      "Epoch 119/200\n",
      "138/138 [==============================] - 1s 9ms/step - loss: 0.0168 - acc: 0.0163\n",
      "Epoch 120/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0168 - acc: 0.0163\n",
      "Epoch 121/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0168 - acc: 0.0163\n",
      "Epoch 122/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0168 - acc: 0.0163\n",
      "Epoch 123/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0167 - acc: 0.0163\n",
      "Epoch 124/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0167 - acc: 0.0168\n",
      "Epoch 125/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0167 - acc: 0.0168\n",
      "Epoch 126/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0167 - acc: 0.0159\n",
      "Epoch 127/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0167 - acc: 0.0168\n",
      "Epoch 128/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0167 - acc: 0.0154\n",
      "Epoch 129/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0167 - acc: 0.0163\n",
      "Epoch 130/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0167 - acc: 0.0168\n",
      "Epoch 131/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0167 - acc: 0.0168\n",
      "Epoch 132/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0167 - acc: 0.0168\n",
      "Epoch 133/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0166 - acc: 0.0159\n",
      "Epoch 134/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0166 - acc: 0.0168\n",
      "Epoch 135/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0166 - acc: 0.0168\n",
      "Epoch 136/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0166 - acc: 0.0154\n",
      "Epoch 137/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0166 - acc: 0.0163\n",
      "Epoch 138/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0166 - acc: 0.0159\n",
      "Epoch 139/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0166 - acc: 0.0159\n",
      "Epoch 140/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0166 - acc: 0.0159\n",
      "Epoch 141/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0166 - acc: 0.0163\n",
      "Epoch 142/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0166 - acc: 0.0168\n",
      "Epoch 143/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0166 - acc: 0.0163\n",
      "Epoch 144/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0165 - acc: 0.0163\n",
      "Epoch 145/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0165 - acc: 0.0168\n",
      "Epoch 146/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0165 - acc: 0.0159\n",
      "Epoch 147/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0165 - acc: 0.0159\n",
      "Epoch 148/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0165 - acc: 0.0168\n",
      "Epoch 149/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0165 - acc: 0.0168\n",
      "Epoch 150/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0165 - acc: 0.0168\n",
      "Epoch 151/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0165 - acc: 0.0163\n",
      "Epoch 152/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0165 - acc: 0.0163\n",
      "Epoch 153/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0165 - acc: 0.0168\n",
      "Epoch 154/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0165 - acc: 0.0163\n",
      "Epoch 155/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0165 - acc: 0.0163\n",
      "Epoch 156/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0165 - acc: 0.0163\n",
      "Epoch 157/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0164 - acc: 0.0168\n",
      "Epoch 158/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0164 - acc: 0.0154\n",
      "Epoch 159/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0164 - acc: 0.0159\n",
      "Epoch 160/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0164 - acc: 0.0163\n",
      "Epoch 161/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0164 - acc: 0.0163\n",
      "Epoch 162/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0164 - acc: 0.0159\n",
      "Epoch 163/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0164 - acc: 0.0177\n",
      "Epoch 164/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0164 - acc: 0.0172\n",
      "Epoch 165/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0164 - acc: 0.0172\n",
      "Epoch 166/200\n",
      "138/138 [==============================] - 1s 8ms/step - loss: 0.0164 - acc: 0.0163\n",
      "Epoch 167/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0164 - acc: 0.0163\n",
      "Epoch 168/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0164 - acc: 0.0163\n",
      "Epoch 169/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0164 - acc: 0.0163\n",
      "Epoch 170/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0164 - acc: 0.0172\n",
      "Epoch 171/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0164 - acc: 0.0163\n",
      "Epoch 172/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0164 - acc: 0.0163\n",
      "Epoch 173/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0163\n",
      "Epoch 174/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0163\n",
      "Epoch 175/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0168\n",
      "Epoch 176/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0163\n",
      "Epoch 177/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0163\n",
      "Epoch 178/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0163\n",
      "Epoch 179/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0159\n",
      "Epoch 180/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0172\n",
      "Epoch 181/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0159\n",
      "Epoch 182/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0163\n",
      "Epoch 183/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0168\n",
      "Epoch 184/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0168\n",
      "Epoch 185/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0163\n",
      "Epoch 186/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0177\n",
      "Epoch 187/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0159\n",
      "Epoch 188/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0168\n",
      "Epoch 189/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0168\n",
      "Epoch 190/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0172\n",
      "Epoch 191/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0177\n",
      "Epoch 192/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0168\n",
      "Epoch 193/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0168\n",
      "Epoch 194/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0177\n",
      "Epoch 195/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0163 - acc: 0.0163\n",
      "Epoch 196/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0162 - acc: 0.0172\n",
      "Epoch 197/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0162 - acc: 0.0177\n",
      "Epoch 198/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0162 - acc: 0.0172\n",
      "Epoch 199/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0162 - acc: 0.0163\n",
      "Epoch 200/200\n",
      "138/138 [==============================] - 1s 7ms/step - loss: 0.0162 - acc: 0.0168\n"
     ]
    }
   ],
   "source": [
    "#這邊是獨立建立五筆autoencoder\n",
    "filea = r\"/Users/yanyaosheng/Desktop/work_keras/final_csv/Browsefox/2bf54e49fc3e09e889819a47d68cf82c975542d3f0a6a41e6306fdb25d74a0c5_3316.csv\"\n",
    "fileb = r\"/Users/yanyaosheng/Desktop/work_keras/final_csv/Browsefox/0c334a2540668f3ac7abf3c7745cd5d0a3622441f17fb9440a5fccd8ed6849b7_3360.csv\"\n",
    "filec = r\"/Users/yanyaosheng/Desktop/work_keras/final_csv/Browsefox/0f3e18c69d7e05f514e9675d5f47bc61423e34dbf94605b97056069925748636_3220.csv\"\n",
    "filed = r\"/Users/yanyaosheng/Desktop/work_keras/final_csv/Browsefox/1ee514676a8305eebf1b945d0c14e94b17e2b950ee2b2cad054623675a033199_3256.csv\"\n",
    "filee = r\"/Users/yanyaosheng/Desktop/work_keras/final_csv/Browsefox/2a4da84497ab3a03e44c23051c0b1b8c4c3c56245919f50e463a793b63a93bec_3332.csv\"\n",
    "k1, k2, k3, k4, k5 = train5modelAE(apa, batch, timestep, numfeature, filea, fileb, filec, filed, filee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 9560d3a4dce1bf484d3affbf3614891b334b713ec17b24562d6b22ca16234b20_3228.csv\n",
      "2 142ef1653fa1e821ca0a6c3ae65b83f630eaf0f6f1b7cebec0755cb40329056b_3348.csv\n",
      "3 fed55bd9dd509d59d240700404c38d20de75e1e72218c83e473e6833cb6eb5b3_3216.csv\n",
      "4 a9d409fb06b301746abe7e7fe9b6a409d681438e24342be92eff6d93dc703add_3332.csv\n",
      "5 e7006a63b1b1b2a63a153b15b830607fa6b5c471ee4610c0369cff879bc5b1c8_3124.csv\n",
      "6 33f56bc524312cf09ed33de7b1fa0b4dd7b387e586c3572a0d04cefad6b7f785_3320.csv\n",
      "7 462df1f85578145417737651e7a684d5cc276fb071ef6539430988a289d04e7b_3264.csv\n",
      "8 19f3145ec6db32fd254136330043c0fd6806d9a9f662a95e630df0b62decacd9_3236.csv\n",
      "9 f1a9ac55c1ef521b1d36d5a69426ef2ed5fbdf02f8bf46d5ab869f4e3af3e22e_3240.csv\n",
      "10 84e4f2b06a094d5638017623b2f8bbf3b88844e03d543bde546b267da1794e20_3188.csv\n",
      "11 a77764ab325d9e9fde6f61b3c0c07de73164c29c80a22511e1cd5966acf7d43b_3328.csv\n",
      "12 7c5cf0827d69c9db358b16a028ee07002eed70efbd89be728593f6f8404fb198_3180.csv\n",
      "13 2a4da84497ab3a03e44c23051c0b1b8c4c3c56245919f50e463a793b63a93bec_3332.csv\n",
      "14 3aa60ef86be648b44c67d4eb61ba978d06b554f82df1c1e65bc039bfb334e657_3292.csv\n",
      "15 f2447ffb1d1660512bbc841a95171d59af53e9df13256b23de4d63600dc078c6_3248.csv\n",
      "16 6bdd4b128448d27a2d7155ef79efc66c2cdc0874b13b332d0cb87374cbfe24ed_3284.csv\n",
      "17 9d31a63a646f5c50bbf54adccc3dd233411250d857b4c694a9a68161781e018e_3188.csv\n",
      "18 5922de4a540424c2a25e83a7f1a845ac0c059ce2dc972b34ad964a452d755088_3304.csv\n",
      "19 d14d1e181d3e8281fdd844be0afbec23050b900e0131cb0f6f8e2b0fd4216764_3232.csv\n",
      "20 570fbc45eff3f108e09aed53d573dbf13954ce86c6d98b9dd0774a87c41e97ab_2848.csv\n",
      "21 796bc3bcabe3d13074c71c31e501c02fcfafca7dbca2a699bd83b1dc459fbbae_3272.csv\n",
      "22 181e802d3436dda661cdc718f66e8e1c5046f1ee036ae7fe5a71cbe97e50cc71_3136.csv\n",
      "23 364d5da92a6e0dcedca39afe83e5b0cdfc3ed0f2938ca5c2320bd7f3e82bb102_3296.csv\n",
      "24 d3880c8e57ebe92731ca39047be72f6c024db2a9166dd9aa9f9602fc1f3aca7c_3180.csv\n",
      "25 e02fe5540c90f259ce62e4657917cb583d92d02ac62c0cd8037cb8c5ef922221_3140.csv\n",
      "26 9a0cb714fa531c99814a0300b548537ac2b62b03df46bf029ad817a857eb7047_3208.csv\n",
      "27 bfa478864bc44ac9d4ee20bf19e52cf3fa73979fbe834ec6c1fbe40e029a850d_3196.csv\n",
      "28 fff6c2061156acb1af15e00e151fac70_3216.csv\n",
      "29 b92832cf79c4018fcd18cf5bbc0e548ecad486b43427b4ce1b07684bd3e52973_3356.csv\n",
      "30 8f96920bfad1e3b411277651a4f2e30aa55aaeaedfc8c071806f846d77bcf204_3292.csv\n",
      "31 d8c6555f8a44ec570a4fcd9bca913dec0bb4b0e4be3fed18ad4118155a80c8c8_3188.csv\n",
      "32 0f3e18c69d7e05f514e9675d5f47bc61423e34dbf94605b97056069925748636_3220.csv\n",
      "33 f102b366ee08e005f527d042591471f2b56f9934ea9c6b7f2741ea515529db96_3236.csv\n",
      "34 d4c8c41816c75f09e182554509bc2dbf9314ca0ed10dc90bd57208222179b965_3348.csv\n",
      "35 9221c35a54d40c6d83cc1f5a3dd94e646ff4c08c3975fb91d3a54f3d966e9835_3292.csv\n",
      "36 ffff67f9d1641a891bb96a2d083a8f20_3168.csv\n",
      "37 90d254df152d5dc20faff9d0c2ad3a2c03b7b3e8263b32e0abf3df75618a0ee1_3320.csv\n",
      "38 dd195914bbea20348c5aaa5e4060a33af81f3d5c6c315bcf1a8f896b1e75c0ea_3332.csv\n",
      "39 50c06aca1493be637de60b4dea6539769ef9df1a6cdc42b59cfad22f3dfa8f89_3292.csv\n",
      "40 e381c72655ced23e7111e01903aa3c514b6e6b5bea2482cff2170936481a0ef9_3268.csv\n",
      "41 2ccbcc893d4baa3f44afadb97e097a14037e83e0972eed309ce5eb3490f6f78b_3324.csv\n",
      "42 0918d3a245a1e39bd32132385a459ef716e37dd542e19e7245e7738b7928aee0_3304.csv\n",
      "43 864a805ee6e75f9ed3ece050b52d9d09628134429c8fc930c956bd0ace583617_3348.csv\n",
      "44 5671b93342f55f40239460d06e0d3224c8760c7f725d54ed4ca7bef60de650de_2896.csv\n",
      "45 4d9aaed043be5debcbdffdf0545d06b0e6400655fbdb86c65b045d9fb23dc20e_3272.csv\n",
      "46 80e87724cdfa90dac1119981744bb7af2d5658c717f154a76d4f10d1a9559740_3208.csv\n",
      "47 ac693a19dd0702644fb633fac68f69d30bc3907622e7beeaf95cd912090dc668_3216.csv\n",
      "48 6caac4d3fad1680bc9a8f1da22e198ddb2c6c7dc40faa02a3df99986823da88b_3236.csv\n",
      "49 9fa6ea0faf66678381af94aceae79abc11b628d9d91dcf99b050b615407ef022_3236.csv\n",
      "50 dafd2278badb1d9e375fb4091ef9a4fdc392dd5f8cddb1738a308053110f0902_3216.csv\n",
      "51 51b967dfb13bc26ab4fa93cd9aceae3ad1aec9a93f5cacb824292078d517bff3_2896.csv\n",
      "52 2bf54e49fc3e09e889819a47d68cf82c975542d3f0a6a41e6306fdb25d74a0c5_3316.csv\n",
      "53 55932e7fec83709d4867b29620cb5f02b54e0f56e4c65c32283ccdd751770774_3244.csv\n",
      "54 2160d853ced0f5d19c35fb0e655e9cdcc9811b7a3259380a8404edeb13a216db_3244.csv\n",
      "55 04a374a1b3c03b073f1822a6ba5b5c45e29bd7e766f9db4e6d1c895571390bb1_3344.csv\n",
      "56 b94a74f8841e73c64ef3e58846e8f196600a11a0f11e0aa7872102f48b4c29ff_3348.csv\n",
      "57 b8419c911c43535caed1af9ef8f1a7d6b49b7a766d4c8bd337e3d4a87b6ddc56_3256.csv\n",
      "58 e213eb6e8a098928374233034dff51b4dfa405b8238429944b35d47407d43c4c_3144.csv\n",
      "59 2bec4b3e8a80b5ca155a6771bd7f6686be40b04f77e7b12cfe59e2523c3004a5_3256.csv\n",
      "60 8590621581580f98c19062dbfba828e851a0001818dd25c424ca7f64ab138dd4_3300.csv\n",
      "it's over son, stop at: 8590621581580f98c19062dbfba828e851a0001818dd25c424ca7f64ab138dd4_3300.csv\n",
      "Train on 4925 samples, validate on 1642 samples\n",
      "Epoch 1/200\n",
      "4925/4925 [==============================] - 8s 2ms/step - loss: 0.2374 - acc: 0.6548 - val_loss: 0.1327 - val_acc: 0.6473\n",
      "Epoch 2/200\n",
      "4925/4925 [==============================] - 2s 329us/step - loss: 0.1004 - acc: 0.6882 - val_loss: 0.1019 - val_acc: 0.6665\n",
      "Epoch 3/200\n",
      "4925/4925 [==============================] - 2s 324us/step - loss: 0.0835 - acc: 0.7009 - val_loss: 0.0947 - val_acc: 0.6802\n",
      "Epoch 4/200\n",
      "4925/4925 [==============================] - 2s 332us/step - loss: 0.0800 - acc: 0.7037 - val_loss: 0.0932 - val_acc: 0.6802\n",
      "Epoch 5/200\n",
      "4925/4925 [==============================] - 2s 336us/step - loss: 0.0793 - acc: 0.7016 - val_loss: 0.0924 - val_acc: 0.6800\n",
      "Epoch 6/200\n",
      "4925/4925 [==============================] - 2s 323us/step - loss: 0.0788 - acc: 0.7016 - val_loss: 0.0920 - val_acc: 0.6800\n",
      "Epoch 7/200\n",
      "4925/4925 [==============================] - 2s 324us/step - loss: 0.0783 - acc: 0.7017 - val_loss: 0.0914 - val_acc: 0.6805\n",
      "Epoch 8/200\n",
      "4925/4925 [==============================] - 2s 328us/step - loss: 0.0778 - acc: 0.7020 - val_loss: 0.0915 - val_acc: 0.6801\n",
      "Epoch 9/200\n",
      "4925/4925 [==============================] - 2s 323us/step - loss: 0.0773 - acc: 0.7019 - val_loss: 0.0907 - val_acc: 0.6800\n",
      "Epoch 10/200\n",
      "4925/4925 [==============================] - 2s 362us/step - loss: 0.0767 - acc: 0.7020 - val_loss: 0.0906 - val_acc: 0.6801\n",
      "Epoch 11/200\n",
      "4925/4925 [==============================] - 2s 389us/step - loss: 0.0762 - acc: 0.7021 - val_loss: 0.0903 - val_acc: 0.6801\n",
      "Epoch 12/200\n",
      "4925/4925 [==============================] - 2s 335us/step - loss: 0.0757 - acc: 0.7021 - val_loss: 0.0896 - val_acc: 0.6804\n",
      "Epoch 13/200\n",
      "4925/4925 [==============================] - 2s 334us/step - loss: 0.0753 - acc: 0.7022 - val_loss: 0.0891 - val_acc: 0.6803\n",
      "Epoch 14/200\n",
      "4925/4925 [==============================] - 2s 341us/step - loss: 0.0749 - acc: 0.7021 - val_loss: 0.0887 - val_acc: 0.6820\n",
      "Epoch 15/200\n",
      "4925/4925 [==============================] - 2s 344us/step - loss: 0.0746 - acc: 0.7022 - val_loss: 0.0889 - val_acc: 0.6804\n",
      "Epoch 16/200\n",
      "4925/4925 [==============================] - 2s 323us/step - loss: 0.0743 - acc: 0.7024 - val_loss: 0.0888 - val_acc: 0.6804\n",
      "Epoch 17/200\n",
      "4925/4925 [==============================] - 2s 342us/step - loss: 0.0740 - acc: 0.7024 - val_loss: 0.0885 - val_acc: 0.6804\n",
      "Epoch 18/200\n",
      "4925/4925 [==============================] - 2s 323us/step - loss: 0.0738 - acc: 0.7023 - val_loss: 0.0883 - val_acc: 0.6804\n",
      "Epoch 19/200\n",
      "4925/4925 [==============================] - 2s 411us/step - loss: 0.0736 - acc: 0.7024 - val_loss: 0.0887 - val_acc: 0.6804\n",
      "Epoch 20/200\n",
      "4925/4925 [==============================] - 3s 569us/step - loss: 0.0734 - acc: 0.7025 - val_loss: 0.0880 - val_acc: 0.6803\n",
      "Epoch 21/200\n",
      "4925/4925 [==============================] - 2s 502us/step - loss: 0.0733 - acc: 0.7026 - val_loss: 0.0881 - val_acc: 0.6804\n",
      "Epoch 22/200\n",
      "4925/4925 [==============================] - 3s 527us/step - loss: 0.0731 - acc: 0.7026 - val_loss: 0.0883 - val_acc: 0.6804\n",
      "Epoch 23/200\n",
      "4925/4925 [==============================] - 2s 482us/step - loss: 0.0730 - acc: 0.7028 - val_loss: 0.0889 - val_acc: 0.6840\n",
      "Epoch 24/200\n",
      "4925/4925 [==============================] - 2s 415us/step - loss: 0.0729 - acc: 0.7031 - val_loss: 0.0878 - val_acc: 0.6818\n",
      "Epoch 25/200\n",
      "4925/4925 [==============================] - 2s 441us/step - loss: 0.0729 - acc: 0.7031 - val_loss: 0.0880 - val_acc: 0.6804\n",
      "Epoch 26/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4925/4925 [==============================] - 2s 395us/step - loss: 0.0728 - acc: 0.7034 - val_loss: 0.0875 - val_acc: 0.6819\n",
      "Epoch 27/200\n",
      "4925/4925 [==============================] - 2s 407us/step - loss: 0.0727 - acc: 0.7038 - val_loss: 0.0880 - val_acc: 0.6804\n",
      "Epoch 28/200\n",
      "4925/4925 [==============================] - 2s 403us/step - loss: 0.0727 - acc: 0.7029 - val_loss: 0.0879 - val_acc: 0.6804\n",
      "Epoch 29/200\n",
      "4925/4925 [==============================] - 2s 407us/step - loss: 0.0726 - acc: 0.7037 - val_loss: 0.0877 - val_acc: 0.6817\n",
      "Epoch 30/200\n",
      "4925/4925 [==============================] - 2s 434us/step - loss: 0.0726 - acc: 0.7039 - val_loss: 0.0878 - val_acc: 0.6819\n",
      "Epoch 31/200\n",
      "4925/4925 [==============================] - 2s 468us/step - loss: 0.0725 - acc: 0.7040 - val_loss: 0.0884 - val_acc: 0.6845\n",
      "Epoch 32/200\n",
      "4925/4925 [==============================] - 2s 464us/step - loss: 0.0725 - acc: 0.7043 - val_loss: 0.0878 - val_acc: 0.6816\n",
      "Epoch 33/200\n",
      "4925/4925 [==============================] - 3s 578us/step - loss: 0.0725 - acc: 0.7043 - val_loss: 0.0878 - val_acc: 0.6805\n",
      "Epoch 34/200\n",
      "4925/4925 [==============================] - 2s 425us/step - loss: 0.0724 - acc: 0.7046 - val_loss: 0.0877 - val_acc: 0.6819\n",
      "Epoch 35/200\n",
      "4925/4925 [==============================] - 2s 390us/step - loss: 0.0724 - acc: 0.7047 - val_loss: 0.0884 - val_acc: 0.6845\n",
      "Epoch 36/200\n",
      "4925/4925 [==============================] - 2s 378us/step - loss: 0.0724 - acc: 0.7047 - val_loss: 0.0881 - val_acc: 0.6828\n",
      "Epoch 37/200\n",
      "4925/4925 [==============================] - 2s 390us/step - loss: 0.0724 - acc: 0.7047 - val_loss: 0.0880 - val_acc: 0.6804\n",
      "Epoch 38/200\n",
      "4925/4925 [==============================] - 2s 375us/step - loss: 0.0724 - acc: 0.7053 - val_loss: 0.0880 - val_acc: 0.6828\n",
      "Epoch 39/200\n",
      "4925/4925 [==============================] - 2s 382us/step - loss: 0.0723 - acc: 0.7049 - val_loss: 0.0880 - val_acc: 0.6828\n",
      "Epoch 40/200\n",
      "4925/4925 [==============================] - 2s 376us/step - loss: 0.0724 - acc: 0.7050 - val_loss: 0.0879 - val_acc: 0.6825\n",
      "Epoch 41/200\n",
      "4925/4925 [==============================] - 2s 385us/step - loss: 0.0724 - acc: 0.7050 - val_loss: 0.0880 - val_acc: 0.6830\n",
      "Epoch 42/200\n",
      "4925/4925 [==============================] - 2s 395us/step - loss: 0.0723 - acc: 0.7051 - val_loss: 0.0881 - val_acc: 0.6827\n",
      "Epoch 43/200\n",
      "4925/4925 [==============================] - 2s 395us/step - loss: 0.0723 - acc: 0.7055 - val_loss: 0.0881 - val_acc: 0.6830\n",
      "Epoch 44/200\n",
      "4925/4925 [==============================] - 2s 409us/step - loss: 0.0723 - acc: 0.7053 - val_loss: 0.0881 - val_acc: 0.6813\n",
      "Epoch 45/200\n",
      "4925/4925 [==============================] - 2s 388us/step - loss: 0.0723 - acc: 0.7052 - val_loss: 0.0879 - val_acc: 0.6817\n",
      "Epoch 46/200\n",
      "4925/4925 [==============================] - 2s 398us/step - loss: 0.0723 - acc: 0.7053 - val_loss: 0.0879 - val_acc: 0.6838\n",
      "Epoch 47/200\n",
      "4925/4925 [==============================] - 2s 394us/step - loss: 0.0723 - acc: 0.7057 - val_loss: 0.0882 - val_acc: 0.6842\n",
      "Epoch 48/200\n",
      "4925/4925 [==============================] - 2s 392us/step - loss: 0.0723 - acc: 0.7053 - val_loss: 0.0885 - val_acc: 0.6845\n",
      "Epoch 49/200\n",
      "4925/4925 [==============================] - 2s 390us/step - loss: 0.0723 - acc: 0.7056 - val_loss: 0.0880 - val_acc: 0.6827\n",
      "Epoch 50/200\n",
      "4925/4925 [==============================] - 2s 416us/step - loss: 0.0723 - acc: 0.7056 - val_loss: 0.0880 - val_acc: 0.6816\n",
      "Epoch 51/200\n",
      "4925/4925 [==============================] - 2s 405us/step - loss: 0.0723 - acc: 0.7054 - val_loss: 0.0880 - val_acc: 0.6838\n",
      "Epoch 52/200\n",
      "4925/4925 [==============================] - 2s 406us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0887 - val_acc: 0.6845\n",
      "Epoch 53/200\n",
      "4925/4925 [==============================] - 2s 394us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0881 - val_acc: 0.6830\n",
      "Epoch 54/200\n",
      "4925/4925 [==============================] - 2s 399us/step - loss: 0.0723 - acc: 0.7056 - val_loss: 0.0883 - val_acc: 0.6842\n",
      "Epoch 55/200\n",
      "4925/4925 [==============================] - 2s 404us/step - loss: 0.0723 - acc: 0.7057 - val_loss: 0.0885 - val_acc: 0.6845\n",
      "Epoch 56/200\n",
      "4925/4925 [==============================] - 2s 406us/step - loss: 0.0723 - acc: 0.7057 - val_loss: 0.0881 - val_acc: 0.6836\n",
      "Epoch 57/200\n",
      "4925/4925 [==============================] - 2s 394us/step - loss: 0.0723 - acc: 0.7057 - val_loss: 0.0881 - val_acc: 0.6831\n",
      "Epoch 58/200\n",
      "4925/4925 [==============================] - 2s 408us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0885 - val_acc: 0.6845\n",
      "Epoch 59/200\n",
      "4925/4925 [==============================] - 2s 383us/step - loss: 0.0723 - acc: 0.7057 - val_loss: 0.0882 - val_acc: 0.6830\n",
      "Epoch 60/200\n",
      "4925/4925 [==============================] - 2s 407us/step - loss: 0.0723 - acc: 0.7058 - val_loss: 0.0886 - val_acc: 0.6845\n",
      "Epoch 61/200\n",
      "4925/4925 [==============================] - 2s 391us/step - loss: 0.0723 - acc: 0.7058 - val_loss: 0.0881 - val_acc: 0.6828\n",
      "Epoch 62/200\n",
      "4925/4925 [==============================] - 2s 385us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0882 - val_acc: 0.6828\n",
      "Epoch 63/200\n",
      "4925/4925 [==============================] - 2s 400us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0887 - val_acc: 0.6845\n",
      "Epoch 64/200\n",
      "4925/4925 [==============================] - 2s 413us/step - loss: 0.0723 - acc: 0.7056 - val_loss: 0.0880 - val_acc: 0.6837\n",
      "Epoch 65/200\n",
      "4925/4925 [==============================] - 2s 384us/step - loss: 0.0723 - acc: 0.7057 - val_loss: 0.0887 - val_acc: 0.6845\n",
      "Epoch 66/200\n",
      "4925/4925 [==============================] - 2s 395us/step - loss: 0.0723 - acc: 0.7058 - val_loss: 0.0883 - val_acc: 0.6845\n",
      "Epoch 67/200\n",
      "4925/4925 [==============================] - 2s 389us/step - loss: 0.0723 - acc: 0.7058 - val_loss: 0.0884 - val_acc: 0.6841\n",
      "Epoch 68/200\n",
      "4925/4925 [==============================] - 2s 381us/step - loss: 0.0723 - acc: 0.7058 - val_loss: 0.0884 - val_acc: 0.6845\n",
      "Epoch 69/200\n",
      "4925/4925 [==============================] - 2s 396us/step - loss: 0.0723 - acc: 0.7057 - val_loss: 0.0882 - val_acc: 0.6830\n",
      "Epoch 70/200\n",
      "4925/4925 [==============================] - 2s 405us/step - loss: 0.0723 - acc: 0.7058 - val_loss: 0.0884 - val_acc: 0.6845\n",
      "Epoch 71/200\n",
      "4925/4925 [==============================] - 2s 398us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0887 - val_acc: 0.6845\n",
      "Epoch 72/200\n",
      "4925/4925 [==============================] - 2s 390us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0880 - val_acc: 0.6837\n",
      "Epoch 73/200\n",
      "4925/4925 [==============================] - 2s 383us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0888 - val_acc: 0.6845\n",
      "Epoch 74/200\n",
      "4925/4925 [==============================] - 2s 385us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0886 - val_acc: 0.6845\n",
      "Epoch 75/200\n",
      "4925/4925 [==============================] - 2s 385us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0882 - val_acc: 0.6830\n",
      "Epoch 76/200\n",
      "4925/4925 [==============================] - 2s 397us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0883 - val_acc: 0.6833\n",
      "Epoch 77/200\n",
      "4925/4925 [==============================] - 2s 415us/step - loss: 0.0723 - acc: 0.7057 - val_loss: 0.0886 - val_acc: 0.6845\n",
      "Epoch 78/200\n",
      "4925/4925 [==============================] - 2s 391us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0883 - val_acc: 0.6830\n",
      "Epoch 79/200\n",
      "4925/4925 [==============================] - 2s 385us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0885 - val_acc: 0.6845\n",
      "Epoch 80/200\n",
      "4925/4925 [==============================] - 2s 387us/step - loss: 0.0723 - acc: 0.7057 - val_loss: 0.0888 - val_acc: 0.6845\n",
      "Epoch 81/200\n",
      "4925/4925 [==============================] - 2s 386us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0888 - val_acc: 0.6845\n",
      "Epoch 82/200\n",
      "4925/4925 [==============================] - 2s 397us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0881 - val_acc: 0.6838\n",
      "Epoch 83/200\n",
      "4925/4925 [==============================] - 2s 387us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0884 - val_acc: 0.6845\n",
      "Epoch 84/200\n",
      "4925/4925 [==============================] - 2s 384us/step - loss: 0.0723 - acc: 0.7062 - val_loss: 0.0886 - val_acc: 0.6845\n",
      "Epoch 85/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4925/4925 [==============================] - 2s 376us/step - loss: 0.0723 - acc: 0.7056 - val_loss: 0.0881 - val_acc: 0.6843\n",
      "Epoch 86/200\n",
      "4925/4925 [==============================] - 2s 370us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0887 - val_acc: 0.6845\n",
      "Epoch 87/200\n",
      "4925/4925 [==============================] - 2s 375us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0884 - val_acc: 0.6841\n",
      "Epoch 88/200\n",
      "4925/4925 [==============================] - 2s 394us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0881 - val_acc: 0.6841\n",
      "Epoch 89/200\n",
      "4925/4925 [==============================] - 2s 373us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0881 - val_acc: 0.6842\n",
      "Epoch 90/200\n",
      "4925/4925 [==============================] - 2s 371us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0882 - val_acc: 0.6840\n",
      "Epoch 91/200\n",
      "4925/4925 [==============================] - 2s 381us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0885 - val_acc: 0.6845\n",
      "Epoch 92/200\n",
      "4925/4925 [==============================] - 2s 372us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0887 - val_acc: 0.6845\n",
      "Epoch 93/200\n",
      "4925/4925 [==============================] - 2s 388us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0883 - val_acc: 0.6836\n",
      "Epoch 94/200\n",
      "4925/4925 [==============================] - 2s 388us/step - loss: 0.0723 - acc: 0.7057 - val_loss: 0.0888 - val_acc: 0.6845\n",
      "Epoch 95/200\n",
      "4925/4925 [==============================] - 2s 388us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0886 - val_acc: 0.6845\n",
      "Epoch 96/200\n",
      "4925/4925 [==============================] - 2s 377us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0882 - val_acc: 0.6830\n",
      "Epoch 97/200\n",
      "4925/4925 [==============================] - 2s 403us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0884 - val_acc: 0.6833\n",
      "Epoch 98/200\n",
      "4925/4925 [==============================] - 2s 394us/step - loss: 0.0723 - acc: 0.7057 - val_loss: 0.0886 - val_acc: 0.6845\n",
      "Epoch 99/200\n",
      "4925/4925 [==============================] - 2s 376us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0882 - val_acc: 0.6840\n",
      "Epoch 100/200\n",
      "4925/4925 [==============================] - 2s 399us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0882 - val_acc: 0.6840\n",
      "Epoch 101/200\n",
      "4925/4925 [==============================] - 2s 374us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0880 - val_acc: 0.6817\n",
      "Epoch 102/200\n",
      "4925/4925 [==============================] - 2s 373us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0881 - val_acc: 0.6837\n",
      "Epoch 103/200\n",
      "4925/4925 [==============================] - 2s 376us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0880 - val_acc: 0.6817\n",
      "Epoch 104/200\n",
      "4925/4925 [==============================] - 2s 388us/step - loss: 0.0723 - acc: 0.7057 - val_loss: 0.0892 - val_acc: 0.6845\n",
      "Epoch 105/200\n",
      "4925/4925 [==============================] - 2s 386us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0882 - val_acc: 0.6840\n",
      "Epoch 106/200\n",
      "4925/4925 [==============================] - 2s 407us/step - loss: 0.0723 - acc: 0.7062 - val_loss: 0.0883 - val_acc: 0.6842\n",
      "Epoch 107/200\n",
      "4925/4925 [==============================] - 2s 452us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0889 - val_acc: 0.6845\n",
      "Epoch 108/200\n",
      "4925/4925 [==============================] - 2s 439us/step - loss: 0.0723 - acc: 0.7058 - val_loss: 0.0883 - val_acc: 0.6837\n",
      "Epoch 109/200\n",
      "4925/4925 [==============================] - 2s 501us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0879 - val_acc: 0.6820\n",
      "Epoch 110/200\n",
      "4925/4925 [==============================] - 2s 402us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0886 - val_acc: 0.6845\n",
      "Epoch 111/200\n",
      "4925/4925 [==============================] - 2s 385us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0882 - val_acc: 0.6831\n",
      "Epoch 112/200\n",
      "4925/4925 [==============================] - 2s 400us/step - loss: 0.0723 - acc: 0.7058 - val_loss: 0.0881 - val_acc: 0.6837\n",
      "Epoch 113/200\n",
      "4925/4925 [==============================] - 2s 382us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0883 - val_acc: 0.6830\n",
      "Epoch 114/200\n",
      "4925/4925 [==============================] - 2s 377us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0882 - val_acc: 0.6830\n",
      "Epoch 115/200\n",
      "4925/4925 [==============================] - 2s 376us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0882 - val_acc: 0.6840\n",
      "Epoch 116/200\n",
      "4925/4925 [==============================] - 2s 382us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0881 - val_acc: 0.6830\n",
      "Epoch 117/200\n",
      "4925/4925 [==============================] - 2s 408us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0888 - val_acc: 0.6845\n",
      "Epoch 118/200\n",
      "4925/4925 [==============================] - 2s 387us/step - loss: 0.0723 - acc: 0.7058 - val_loss: 0.0884 - val_acc: 0.6843\n",
      "Epoch 119/200\n",
      "4925/4925 [==============================] - 2s 407us/step - loss: 0.0723 - acc: 0.7058 - val_loss: 0.0885 - val_acc: 0.6845\n",
      "Epoch 120/200\n",
      "4925/4925 [==============================] - 2s 401us/step - loss: 0.0723 - acc: 0.7062 - val_loss: 0.0881 - val_acc: 0.6827\n",
      "Epoch 121/200\n",
      "4925/4925 [==============================] - 2s 408us/step - loss: 0.0723 - acc: 0.7058 - val_loss: 0.0883 - val_acc: 0.6842\n",
      "Epoch 122/200\n",
      "4925/4925 [==============================] - 2s 429us/step - loss: 0.0723 - acc: 0.7058 - val_loss: 0.0882 - val_acc: 0.6840\n",
      "Epoch 123/200\n",
      "4925/4925 [==============================] - 2s 394us/step - loss: 0.0723 - acc: 0.7062 - val_loss: 0.0883 - val_acc: 0.6843\n",
      "Epoch 124/200\n",
      "4925/4925 [==============================] - 2s 408us/step - loss: 0.0723 - acc: 0.7062 - val_loss: 0.0882 - val_acc: 0.6835\n",
      "Epoch 125/200\n",
      "4925/4925 [==============================] - 2s 395us/step - loss: 0.0723 - acc: 0.7058 - val_loss: 0.0885 - val_acc: 0.6845\n",
      "Epoch 126/200\n",
      "4925/4925 [==============================] - 2s 383us/step - loss: 0.0723 - acc: 0.7058 - val_loss: 0.0882 - val_acc: 0.6841\n",
      "Epoch 127/200\n",
      "4925/4925 [==============================] - 2s 388us/step - loss: 0.0723 - acc: 0.7058 - val_loss: 0.0886 - val_acc: 0.6845\n",
      "Epoch 128/200\n",
      "4925/4925 [==============================] - 2s 448us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0881 - val_acc: 0.6838\n",
      "Epoch 129/200\n",
      "4925/4925 [==============================] - 2s 420us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0880 - val_acc: 0.6828\n",
      "Epoch 130/200\n",
      "4925/4925 [==============================] - 2s 410us/step - loss: 0.0723 - acc: 0.7058 - val_loss: 0.0881 - val_acc: 0.6837\n",
      "Epoch 131/200\n",
      "4925/4925 [==============================] - 2s 428us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0885 - val_acc: 0.6845\n",
      "Epoch 132/200\n",
      "4925/4925 [==============================] - 2s 417us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0884 - val_acc: 0.6843\n",
      "Epoch 133/200\n",
      "4925/4925 [==============================] - 2s 405us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0884 - val_acc: 0.6845\n",
      "Epoch 134/200\n",
      "4925/4925 [==============================] - 2s 394us/step - loss: 0.0723 - acc: 0.7062 - val_loss: 0.0881 - val_acc: 0.6840\n",
      "Epoch 135/200\n",
      "4925/4925 [==============================] - 2s 416us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0889 - val_acc: 0.6845\n",
      "Epoch 136/200\n",
      "4925/4925 [==============================] - 2s 410us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0882 - val_acc: 0.6830\n",
      "Epoch 137/200\n",
      "4925/4925 [==============================] - 2s 393us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0882 - val_acc: 0.6828\n",
      "Epoch 138/200\n",
      "4925/4925 [==============================] - 2s 406us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0882 - val_acc: 0.6831\n",
      "Epoch 139/200\n",
      "4925/4925 [==============================] - 2s 419us/step - loss: 0.0723 - acc: 0.7057 - val_loss: 0.0885 - val_acc: 0.6845\n",
      "Epoch 140/200\n",
      "4925/4925 [==============================] - 2s 420us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0886 - val_acc: 0.6843\n",
      "Epoch 141/200\n",
      "4925/4925 [==============================] - 2s 405us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0882 - val_acc: 0.6827\n",
      "Epoch 142/200\n",
      "4925/4925 [==============================] - 2s 437us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0882 - val_acc: 0.6841\n",
      "Epoch 143/200\n",
      "4925/4925 [==============================] - 2s 429us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0891 - val_acc: 0.6845\n",
      "Epoch 144/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4925/4925 [==============================] - 2s 379us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0886 - val_acc: 0.6845\n",
      "Epoch 145/200\n",
      "4925/4925 [==============================] - 2s 370us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0882 - val_acc: 0.6841\n",
      "Epoch 146/200\n",
      "4925/4925 [==============================] - 2s 382us/step - loss: 0.0723 - acc: 0.7062 - val_loss: 0.0887 - val_acc: 0.6845\n",
      "Epoch 147/200\n",
      "4925/4925 [==============================] - 2s 372us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0880 - val_acc: 0.6828\n",
      "Epoch 148/200\n",
      "4925/4925 [==============================] - 2s 367us/step - loss: 0.0723 - acc: 0.7062 - val_loss: 0.0886 - val_acc: 0.6845\n",
      "Epoch 149/200\n",
      "4925/4925 [==============================] - 2s 369us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0882 - val_acc: 0.6830\n",
      "Epoch 150/200\n",
      "4925/4925 [==============================] - 2s 370us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0879 - val_acc: 0.6820\n",
      "Epoch 151/200\n",
      "4925/4925 [==============================] - 2s 370us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0884 - val_acc: 0.6843\n",
      "Epoch 152/200\n",
      "4925/4925 [==============================] - 2s 375us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0882 - val_acc: 0.6830\n",
      "Epoch 153/200\n",
      "4925/4925 [==============================] - 2s 366us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0886 - val_acc: 0.6845\n",
      "Epoch 154/200\n",
      "4925/4925 [==============================] - 2s 372us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0888 - val_acc: 0.6845\n",
      "Epoch 155/200\n",
      "4925/4925 [==============================] - 2s 387us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0881 - val_acc: 0.6825\n",
      "Epoch 156/200\n",
      "4925/4925 [==============================] - 2s 389us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0883 - val_acc: 0.6830\n",
      "Epoch 157/200\n",
      "4925/4925 [==============================] - 2s 419us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0887 - val_acc: 0.6845\n",
      "Epoch 158/200\n",
      "4925/4925 [==============================] - 2s 384us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0885 - val_acc: 0.6845\n",
      "Epoch 159/200\n",
      "4925/4925 [==============================] - 2s 379us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0888 - val_acc: 0.6845\n",
      "Epoch 160/200\n",
      "4925/4925 [==============================] - 2s 367us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0883 - val_acc: 0.6833\n",
      "Epoch 161/200\n",
      "4925/4925 [==============================] - 2s 380us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0883 - val_acc: 0.6845\n",
      "Epoch 162/200\n",
      "4925/4925 [==============================] - 2s 386us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0883 - val_acc: 0.6831\n",
      "Epoch 163/200\n",
      "4925/4925 [==============================] - 2s 392us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0885 - val_acc: 0.6842\n",
      "Epoch 164/200\n",
      "4925/4925 [==============================] - 2s 386us/step - loss: 0.0723 - acc: 0.7058 - val_loss: 0.0888 - val_acc: 0.6845\n",
      "Epoch 165/200\n",
      "4925/4925 [==============================] - 2s 386us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0881 - val_acc: 0.6828\n",
      "Epoch 166/200\n",
      "4925/4925 [==============================] - 2s 389us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0885 - val_acc: 0.6845\n",
      "Epoch 167/200\n",
      "4925/4925 [==============================] - 2s 370us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0892 - val_acc: 0.6845\n",
      "Epoch 168/200\n",
      "4925/4925 [==============================] - 2s 367us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0880 - val_acc: 0.6840\n",
      "Epoch 169/200\n",
      "4925/4925 [==============================] - 2s 366us/step - loss: 0.0723 - acc: 0.7058 - val_loss: 0.0881 - val_acc: 0.6839\n",
      "Epoch 170/200\n",
      "4925/4925 [==============================] - 2s 367us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0884 - val_acc: 0.6845\n",
      "Epoch 171/200\n",
      "4925/4925 [==============================] - 2s 397us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0890 - val_acc: 0.6845\n",
      "Epoch 172/200\n",
      "4925/4925 [==============================] - 2s 381us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0884 - val_acc: 0.6845\n",
      "Epoch 173/200\n",
      "4925/4925 [==============================] - 2s 373us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0880 - val_acc: 0.6840\n",
      "Epoch 174/200\n",
      "4925/4925 [==============================] - 2s 371us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0884 - val_acc: 0.6830\n",
      "Epoch 175/200\n",
      "4925/4925 [==============================] - 2s 395us/step - loss: 0.0723 - acc: 0.7058 - val_loss: 0.0883 - val_acc: 0.6827\n",
      "Epoch 176/200\n",
      "4925/4925 [==============================] - 2s 394us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0883 - val_acc: 0.6830\n",
      "Epoch 177/200\n",
      "4925/4925 [==============================] - 2s 414us/step - loss: 0.0723 - acc: 0.7058 - val_loss: 0.0884 - val_acc: 0.6843\n",
      "Epoch 178/200\n",
      "4925/4925 [==============================] - 2s 388us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0881 - val_acc: 0.6824\n",
      "Epoch 179/200\n",
      "4925/4925 [==============================] - 2s 370us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0884 - val_acc: 0.6830\n",
      "Epoch 180/200\n",
      "4925/4925 [==============================] - 2s 375us/step - loss: 0.0723 - acc: 0.7058 - val_loss: 0.0883 - val_acc: 0.6843\n",
      "Epoch 181/200\n",
      "4925/4925 [==============================] - 2s 383us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0881 - val_acc: 0.6842\n",
      "Epoch 182/200\n",
      "4925/4925 [==============================] - 2s 371us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0880 - val_acc: 0.6836\n",
      "Epoch 183/200\n",
      "4925/4925 [==============================] - 2s 371us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0879 - val_acc: 0.6828\n",
      "Epoch 184/200\n",
      "4925/4925 [==============================] - 2s 386us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0890 - val_acc: 0.6845\n",
      "Epoch 185/200\n",
      "4925/4925 [==============================] - 2s 393us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0885 - val_acc: 0.6845\n",
      "Epoch 186/200\n",
      "4925/4925 [==============================] - 2s 403us/step - loss: 0.0723 - acc: 0.7062 - val_loss: 0.0883 - val_acc: 0.6836\n",
      "Epoch 187/200\n",
      "4925/4925 [==============================] - 2s 396us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0885 - val_acc: 0.6845\n",
      "Epoch 188/200\n",
      "4925/4925 [==============================] - 2s 388us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0883 - val_acc: 0.6830\n",
      "Epoch 189/200\n",
      "4925/4925 [==============================] - 2s 388us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0881 - val_acc: 0.6836\n",
      "Epoch 190/200\n",
      "4925/4925 [==============================] - 2s 412us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0882 - val_acc: 0.6836\n",
      "Epoch 191/200\n",
      "4925/4925 [==============================] - 2s 391us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0883 - val_acc: 0.6831\n",
      "Epoch 192/200\n",
      "4925/4925 [==============================] - 2s 394us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0882 - val_acc: 0.6828\n",
      "Epoch 193/200\n",
      "4925/4925 [==============================] - 2s 415us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0880 - val_acc: 0.6838\n",
      "Epoch 194/200\n",
      "4925/4925 [==============================] - 2s 388us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0880 - val_acc: 0.6819\n",
      "Epoch 195/200\n",
      "4925/4925 [==============================] - 2s 382us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0881 - val_acc: 0.6837\n",
      "Epoch 196/200\n",
      "4925/4925 [==============================] - 2s 394us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0882 - val_acc: 0.6827\n",
      "Epoch 197/200\n",
      "4925/4925 [==============================] - 2s 397us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0883 - val_acc: 0.6830\n",
      "Epoch 198/200\n",
      "4925/4925 [==============================] - 2s 395us/step - loss: 0.0723 - acc: 0.7061 - val_loss: 0.0886 - val_acc: 0.6845\n",
      "Epoch 199/200\n",
      "4925/4925 [==============================] - 2s 385us/step - loss: 0.0723 - acc: 0.7059 - val_loss: 0.0885 - val_acc: 0.6840\n",
      "Epoch 200/200\n",
      "4925/4925 [==============================] - 2s 396us/step - loss: 0.0723 - acc: 0.7060 - val_loss: 0.0892 - val_acc: 0.6845\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===\n",
      "8370580ba494843b468b3b64b9536314c189f58c943c7650e4edc83c9337decd_3104.csv\n",
      "481.52021186815824\n",
      "===\n",
      "5492a866ef73970273f463c54fa3097973c8035c5d6a42165af50bf6b7b142e1_3208.csv\n",
      "613.8096121722319\n",
      "===\n",
      "fff67ad46d9fdd282d9c6da6cc9950e0_3168.csv\n",
      "696.7611276935658\n",
      "===\n",
      "43cf10fbebf225aafd0bb7670bcf538d425e7276f095b667a8e5094ee0a6d2ef_3276.csv\n",
      "621.4819029620166\n",
      "===\n",
      "0c334a2540668f3ac7abf3c7745cd5d0a3622441f17fb9440a5fccd8ed6849b7_3360.csv\n",
      "689.7326153874585\n",
      "===\n",
      "f456d967733249bd69e4e7ccdf556bb8b7d032e12268bb22a63848d59824ca0a_3224.csv\n",
      "639.0129446168709\n",
      "===\n",
      "87ed74058b66bd7b9da46496cbbda5ffe165f9af426d34dbac8b69c7878b5eef_3212.csv\n",
      "582.8158051139719\n",
      "===\n",
      "c438adabebbcb449b876c4f848276a14bffca76ada823f0a986771500d59f7d2_3308.csv\n",
      "453.6606032287279\n",
      "===\n",
      "3797f9008d63d93838f4899b03ad0471dd8417353c331e42c244cac0b6c021bd_3320.csv\n",
      "0.35416760080352533\n",
      "===\n",
      "1ee514676a8305eebf1b945d0c14e94b17e2b950ee2b2cad054623675a033199_3256.csv\n",
      "650.318615768254\n",
      "===\n",
      "b1d5675eb7e4ed7d032cfac03ef926e632524c411ef7d915983dddfa1a1cb6fe_3296.csv\n",
      "589.1322324666957\n",
      "===\n",
      "33089bad41eb6562994b5ac8d254500d92b07717d18e70ba022c98a6199133be_3284.csv\n",
      "608.574589263317\n",
      "===\n",
      "d3817de6f910a4a5fc4d593428c24d9603a37f4be9adb4d11fae011dd346866e_3208.csv\n",
      "566.1726253148049\n",
      "===\n",
      "5190207e22082f5a17029a10c998924691925af623b30cdc117320e71d3c20e8_3260.csv\n",
      "608.574589263317\n"
     ]
    }
   ],
   "source": [
    "filenamelist =  r\"/Users/yanyaosheng/Desktop/work_keras/final_csv/Browsefox/\"\n",
    "stoppoint = 60\n",
    "aaa = 0\n",
    "for filename in listdir(filenamelist):\n",
    "    test_x, scaler,y = data_preprocess(filenamelist+filename, timestep)\n",
    "    test_x = test_x.reshape(test_x.shape[0], timestep, numfeature)\n",
    "    if aaa == 0:\n",
    "        new_test_x = test_x\n",
    "        new_y = y\n",
    "    else:\n",
    "        new_test_x = np.concatenate([test_x, new_test_x],0)\n",
    "        new_y = np.concatenate([y, new_y],0)\n",
    "    aaa += 1\n",
    "    print str(aaa)+\" \"+filename\n",
    "    if aaa == stoppoint:\n",
    "        print \"it's over son, stop at: \"+filename\n",
    "        break\n",
    "################################################################\n",
    "#上述把它資料夾內的檔案通通變成一個檔案格式(xxx, 16, 25)\n",
    "################################################################\n",
    "kkk = postboosting(new_test_x, timestep, numfeature, apa, batch)\n",
    "################################################################\n",
    "#kkk為訓練好的boosting檔案\n",
    "################################################################\n",
    "filepathlist = r\"/Users/yanyaosheng/Desktop/work_keras/final_csv/Browsefox/\"\n",
    "a = []\n",
    "for ii in range( aaa, (len(listdir(filenamelist))) ):\n",
    "    test_x, scaler,y = data_preprocess(filepathlist+listdir(filenamelist)[ii], timestep)\n",
    "    test_x = test_x.reshape(test_x.shape[0], timestep, numfeature)\n",
    "    yhatk1 = k1.predict(test_x)\n",
    "    yhatk2 = k2.predict(test_x)\n",
    "    yhatk3 = k3.predict(test_x)\n",
    "    yhatk4 = k4.predict(test_x)\n",
    "    yhatk5 = k5.predict(test_x)\n",
    "    inputofdense = np.concatenate((yhatk1, yhatk2, yhatk3, yhatk4, yhatk5), 1).reshape(test_x.shape[0], 5, timestep * numfeature,1)\n",
    "    yhat = kkk.predict(inputofdense)\n",
    "    yhat = yhat.reshape(yhat.shape[0], timestep * numfeature)\n",
    "    yhat = scaler.inverse_transform(yhat)\n",
    "    print\"===\"\n",
    "    rmse = np.sqrt(np.mean(((yhat - y) ** 2),axis=1))\n",
    "    print listdir(filenamelist)[ii]\n",
    "    print np.mean(rmse)\n",
    "    a.append(listdir(filenamelist)[ii])\n",
    "    a.append(np.mean(rmse))\n",
    "    \n",
    "thefile = open('test.txt', 'w')\n",
    "for item in a:\n",
    "    thefile.write(\"%s\\n\" % item)\n",
    "thefile.close()\n",
    "################################################################\n",
    "#上述是將單一家族資料夾內的所有資料都去跑boosting alg.並把rmse記錄起來 寫成檔案\n",
    "################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608370580ba494843b468b3b64b9536314c189f58c943c7650e4edc83c9337decd_3104.csv\n",
      "44/44 [==============================] - 0s 2ms/step\n",
      "60, score: 0.1065184065902775, acc= 0.7313636473634026\n",
      "615492a866ef73970273f463c54fa3097973c8035c5d6a42165af50bf6b7b142e1_3208.csv\n",
      "138/138 [==============================] - 0s 2ms/step\n",
      "61, score: 0.03257756384656481, acc= 0.7667209985463516\n",
      "62fff67ad46d9fdd282d9c6da6cc9950e0_3168.csv\n",
      "72/72 [==============================] - 0s 2ms/step\n",
      "62, score: 0.12201648236562808, acc= 0.6511805471446779\n",
      "6343cf10fbebf225aafd0bb7670bcf538d425e7276f095b667a8e5094ee0a6d2ef_3276.csv\n",
      "146/146 [==============================] - 0s 2ms/step\n",
      "63, score: 0.03541917537581431, acc= 0.8040068508827523\n",
      "640c334a2540668f3ac7abf3c7745cd5d0a3622441f17fb9440a5fccd8ed6849b7_3360.csv\n",
      "9/9 [==============================] - 0s 2ms/step\n",
      "64, score: 0.11992266029119492, acc= 0.7122222251362271\n",
      "65f456d967733249bd69e4e7ccdf556bb8b7d032e12268bb22a63848d59824ca0a_3224.csv\n",
      "146/146 [==============================] - 0s 1ms/step\n",
      "65, score: 0.035441752612488726, acc= 0.8040068508827523\n",
      "6687ed74058b66bd7b9da46496cbbda5ffe165f9af426d34dbac8b69c7878b5eef_3212.csv\n",
      "138/138 [==============================] - 0s 1ms/step\n",
      "66, score: 0.04704856483832649, acc= 0.7690398433934087\n",
      "67c438adabebbcb449b876c4f848276a14bffca76ada823f0a986771500d59f7d2_3308.csv\n",
      "72/72 [==============================] - 0s 2ms/step\n",
      "67, score: 0.09929770200202863, acc= 0.7144791608055433\n",
      "683797f9008d63d93838f4899b03ad0471dd8417353c331e42c244cac0b6c021bd_3320.csv\n",
      "1/1 [==============================] - 0s 2ms/step\n",
      "68, score: 0.1313948631286621, acc= 0.7400000095367432\n",
      "691ee514676a8305eebf1b945d0c14e94b17e2b950ee2b2cad054623675a033199_3256.csv\n",
      "138/138 [==============================] - 0s 2ms/step\n",
      "69, score: 0.035989018076139946, acc= 0.8033876829389213\n",
      "70b1d5675eb7e4ed7d032cfac03ef926e632524c411ef7d915983dddfa1a1cb6fe_3296.csv\n",
      "138/138 [==============================] - 0s 2ms/step\n",
      "70, score: 0.04094664763281311, acc= 0.8033876829389213\n",
      "7133089bad41eb6562994b5ac8d254500d92b07717d18e70ba022c98a6199133be_3284.csv\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "71, score: 0.11735428124666214, acc= 0.7335000157356262\n",
      "72d3817de6f910a4a5fc4d593428c24d9603a37f4be9adb4d11fae011dd346866e_3208.csv\n",
      "13/13 [==============================] - 0s 2ms/step\n",
      "72, score: 0.10875505610154225, acc= 0.7142307712481573\n",
      "735190207e22082f5a17029a10c998924691925af623b30cdc117320e71d3c20e8_3260.csv\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "73, score: 0.11735428124666214, acc= 0.7335000157356262\n"
     ]
    }
   ],
   "source": [
    "for ii in range( aaa, (len(listdir(filenamelist))) ):\n",
    "    test_x, scaler,y = data_preprocess(filepathlist+listdir(filenamelist)[ii], timestep)\n",
    "    test_x = test_x.reshape(test_x.shape[0], timestep, numfeature)\n",
    "    yhatk1 = k1.predict(test_x)\n",
    "    yhatk2 = k2.predict(test_x)\n",
    "    yhatk3 = k3.predict(test_x)\n",
    "    yhatk4 = k4.predict(test_x)\n",
    "    yhatk5 = k5.predict(test_x)\n",
    "    inputofdense = np.concatenate((yhatk1, yhatk2, yhatk3, yhatk4, yhatk5), 1).reshape(test_x.shape[0], 5, timestep * numfeature,1)\n",
    "    print str(ii)+listdir(filenamelist)[ii]\n",
    "    #test_x.shape[0], 1, numfeature*timestep, 1\n",
    "    #y = 44, 400\n",
    "    #inputofdense = test_x.shape[0], 5, timestep * numfeature,1\n",
    "    #44, 5, 400, 1\n",
    "    score, acc = kkk.evaluate(inputofdense, test_x.reshape(y.shape[0], 1, 400, 1),batch_size = 1)\n",
    "    print str(ii)+\", score: \"+str(score)+\", acc= \"+str(acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146/146 [==============================] - 0s 1ms/step\n",
      "\n",
      "0, 2bf54e49fc3e09e889819a47d68cf82c975542d3f0a6a41e6306fdb25d74a0c5_3316.csv, score: 0.03134853200195995, acc= 0.7672773807832639\n",
      "==============\n",
      "9/9 [==============================] - 0s 3ms/step\n",
      "\n",
      "1, 0c334a2540668f3ac7abf3c7745cd5d0a3622441f17fb9440a5fccd8ed6849b7_3360.csv, score: 0.11695306996504466, acc= 0.7130555576748319\n",
      "==============\n",
      "146/146 [==============================] - 0s 1ms/step\n",
      "\n",
      "2, e381c72655ced23e7111e01903aa3c514b6e6b5bea2482cff2170936481a0ef9_3268.csv, score: 0.03742903941757467, acc= 0.8039726043400699\n",
      "==============\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "\n",
      "3, 6caac4d3fad1680bc9a8f1da22e198ddb2c6c7dc40faa02a3df99986823da88b_3236.csv, score: 0.12840998669465384, acc= 0.73416668176651\n",
      "==============\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filepathlist = r\"/Users/yanyaosheng/Desktop/work_keras/final_csv/Browsefox/\"\n",
    "ooo = [r\"2bf54e49fc3e09e889819a47d68cf82c975542d3f0a6a41e6306fdb25d74a0c5_3316.csv\",\n",
    "       r\"0c334a2540668f3ac7abf3c7745cd5d0a3622441f17fb9440a5fccd8ed6849b7_3360.csv\",\n",
    "       r\"e381c72655ced23e7111e01903aa3c514b6e6b5bea2482cff2170936481a0ef9_3268.csv\",\n",
    "       r\"6caac4d3fad1680bc9a8f1da22e198ddb2c6c7dc40faa02a3df99986823da88b_3236.csv\",\n",
    "       ]\n",
    "\n",
    "for ii in range(len(ooo)):\n",
    "    test_x, scaler,y = data_preprocess(filepathlist+ooo[ii], timestep)\n",
    "    test_x = test_x.reshape(test_x.shape[0], timestep, numfeature)\n",
    "    yhatk1 = k1.predict(test_x)\n",
    "    yhatk2 = k2.predict(test_x)\n",
    "    yhatk3 = k3.predict(test_x)\n",
    "    yhatk4 = k4.predict(test_x)\n",
    "    yhatk5 = k5.predict(test_x)\n",
    "    inputofdense = np.concatenate((yhatk1, yhatk2, yhatk3, yhatk4, yhatk5), 1).reshape(test_x.shape[0], 5, timestep * numfeature,1)\n",
    "    #print str(ii)+ooo[ii]\n",
    "    #test_x.shape[0], 1, numfeature*timestep, 1\n",
    "    #y = 44, 400\n",
    "    #inputofdense = test_x.shape[0], 5, timestep * numfeature,1\n",
    "    #44, 5, 400, 1\n",
    "    score, acc = kkk.evaluate(inputofdense, test_x.reshape(y.shape[0], 1, 400, 1),batch_size = 1)\n",
    "    print \"\\n\"+str(ii)+\", \"+ooo[ii]+\", score: \"+str(score)+\", acc= \"+str(acc)+\"\\n==============\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name np_utils",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-51db091c23cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/venv/malawareclassification/lib/python2.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/venv/malawareclassification/lib/python2.7/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name np_utils"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from keras.models import load_model\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from os import listdir\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import RepeatVector\n",
    "from keras.layers import Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import model_from_yaml\n",
    "import os\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name np_utils",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c74e2bd4ca71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/user/venv/malawareclassification/lib/python2.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/venv/malawareclassification/lib/python2.7/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name np_utils"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name np_utils",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5646143afa4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/user/venv/malawareclassification/lib/python2.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/venv/malawareclassification/lib/python2.7/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name np_utils"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempted relative import in non-package",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7d749c0194e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: Attempted relative import in non-package"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from . import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
